\chapter{Tools and Methods}
\label{ch:tools}

Evaluation of the capabilities of DUNE/LBNF to realize the scientific program envisioned requires a detailed understanding of the experimental signatures of the relevant physical processes, the response of detection elements, and the performance of calibration systems and event reconstruction and other tools that enable analysis of data from the DUNE detectors.  It is the aim of this chapter to introduce the network of calibration, simulation, and reconstruction tools that form the basis for the demonstration of science capabilities presented in the chapters that follow.  The presentation here covers general components, namely those that are commonly utilized across the science program, although many of these are geared toward application to the long-baseline oscillation physics at the heart of this program.  Other tools and methods developed for specific physics applications are described in the corresponding chapters that follow.

Where appropriate, the performance of reconstruction tools and algorithms is quantified.  Some of these characterizations form the basis for parameterized-response simulations used by physics sensitivity studies that have not yet advanced to the level of analysis of fully reconstructed simulated data.  They also serve as metrics that allow linkages to be drawn between detector configuration specifications and physics sensitivity.

Another critical role for the simulation and reconstruction tools described in this chapter, implicit above, is to enable detailed study of sources of systematic error that can affect physics capability, which can also lead to the development of mitigation strategies.  Thus, where possible, assessments of systematic uncertainties in the modeling of LBNF/DUNE conditions and performance are presented.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Monte Carlo Simulations}
\label{sec:tools-mc}

%Liquid argon time projection chambers (LArTPCs) provide a robust and elegant method for measuring the properties of neutrino interactions above a few tens of MeV by providing 3D event imaging with excellent spatial and energy resolution. For simulations, we produce large samples of Monte Carlo events (Monte Carlo Challenge - MCC) on a regular basis. The purpose is to test the latest simulation and reconstruction chain as well as the grid job submission tools and sam/tape interfaces and provide standard samples for various physics working groups to analyze. All the MCC samples are produced using the batch tool larbatch~\cite{ref:larbatch}.

%MCC1.0 was produced in January 2015 with three 35t samples. We include far detector and protoDUNE samples in later MCCs and incorporate more sophisticated detector simulation and reconstruction software. A more recent MC production MCC6.0, which was produced in May 2016, consists of 50 samples using the 35t, far detector and protoDUNE single-phase geometries. MCC6.0 was produced using LArSoft v05\_09\_01. The locations of the samples can be found in Ref~\cite{ref:mcc6}. 



Many physics processes are simulated in the DUNE \dword{fd}; these include beam neutrinos, atmospheric neutrinos, \dword{snb} neutrinos, proton decays and cosmogenic events. Figure~\ref{fig:dune_tpc} shows a portion of the DUNE \single TPC consisting of \dword{apa}s and \dword{cpa}s.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.7\textwidth]{DUNE_TPC.PNG}
\caption{A portion of DUNE single-phase TPC is shown. Four separate drift regions are separated by APA and CPA. }
\label{fig:dune_tpc}
\end{figure}

%In MCC6.0, we simulated 3 types of beam neutrinos (unoscillated $\nu_\mu$'s, fully-oscillated $\nu_{e}$'s and fully-oscillated $\nu_\tau$'s), atmospheric neutrinos, supernova neutrinos, proton decays and cosmogenic events in the far detector. 
To save processing time, all the \dword{fd} samples except the cosmogenics sample were simulated using a smaller version of the full \nominalmodsize far \dword{detmodule} geometry. This geometry is 13.9 m long, 12 m high and 13.3 m wide, which consists of 12 \dword{apa}s and 24 \dword{cpa}s. % anne fixed TPCs. 
%In the default configuration, the TPC wire spacing is 5 mm, the wire angle is 36$^{\circ}$ and the neutrino beam is parallel to the wire planes. 
Figure~\ref{fig:dune_apa} shows the detailed structure of an \dword{apa}. %We also generated special samples where the wire spacing is 3 mm or the wire angle is 45$^{\circ}$ or the neutrino beam is perpendicular to the wire planes for the detector optimization studies. 

\begin{figure}[!ht]
\centering
\includegraphics[width=0.7\textwidth]{DUNE_APA.PNG}
\caption{The detailed structure of the \dword{apa} is shown. Each \dword{apa} consists of four wrapped induction wire planes and two collection wire planes.
The \dword{pd} is sandwiched between the two collection wire planes.}
\label{fig:dune_apa}
\end{figure}

For the simulation chain, each sample is simulated in three steps: generation (gen), {\sc geant4} tracking (g4), TPC signal simulation, and digitization (detsim). The first step is unique for each sample while the second and the third steps are mostly identical for all samples. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Neutrino Flux Modeling}
\label{sec:tools-mc-flux}
%{\it Assigned to:} {\bf Laura Fields} with contributions from Zarko Pavlovic and Luke Pickering.

Neutrino fluxes were generated using G4LBNF, a \textsc{Geant}4\xspace-based simulation of the LBNF neutrino beam.  The simulation was configured to use a detailed description of the LBNF optimized beam design~\cite{optimizedbeamcdr}.  That design starts with a 1.2 MW, 120 GeV primary proton beam that impinges on a 2.2 m long, 16 mm diameter cylindrical graphite target.  Hadrons produced in the target are focused by three magnetic horns operated with 300 kA currents.  The target chase is followed by a 194~m helium-filled decay pipe and a hadron absorber.  The focusing horns can be operated in forward or reverse current configurations, creating neutrino and antineutrino beams, respectively.   

\begin{dunefigure}[Visualization of the focusing system as simulated in g4lbnf]{fig:beam_vis}
{Visualization of the focusing system as simulated in g4lbnf.}
    \includegraphics[width=0.7\textwidth]{Beamline_optimized_sept2017.pdf}\end{dunefigure}

The optimized LBNF neutrino beam design is the result of several years of effort by LBNF and DUNE to identify a focusing system optimized to DUNE's long-baseline physics goals.  This process began with a genetic algorithm that scanned simulations of many different horn and target geometries to identify those that produced the optimal sensitivity to \dword{cpv}.  The specific metric used was estimated sensitivity to 75\% of \dword{cp} phase space after \ktmwyr{300}  %kT MW years 
of exposure, taking into account the number and neutrino spectra of all neutrino flavors. The resulting beam effectively optimized flux at the first and second oscillation maxima, which also benefits measurements of other oscillation parameters.  The output of the genetic algorithm was a simple design including horn conductor and target shapes.  This design was transformed into a detailed conceptual design by LBNF engineers, and iterated with DUNE physicists to ensure that engineering changes had minimal impact on physics performance.  Relative to the previous NuMI-like design, the optimized design reduces the time to three-sigma coverage of 75\% of \dword{cp} phase space by 42\%, which is equivalent to increasing the mass of the far detector by 70\%.  It also substantially increases sensitivity to the mass hierarchy and improves projected resolution to quantities such as $\sin^22\theta_{13}$ and $\sin^2\theta_{23}$~\cite{fields_doc_2901}.        

\subsubsection{On-axis Neutrino Flux and Uncertainties}

\fixme{in figure 4, near or far? the captions don't match. anne}
\begin{dunefigure}[Neutrino fluxes at the far detector]{fig:flux_flavor}
{Predicted neutrino fluxes at the near detector for neutrino mode (left) and antineutrino mode (right). From top to bottom shown are muon neutrino, muon antineutrino, electron neutrino, and electron antineutrino fluxes.}
    \includegraphics[width=0.35\textwidth]{graphics/dune_neutrino_nd_numu.pdf}
     \includegraphics[width=0.35\textwidth]{graphics/dune_antineutrino_nd_numu.pdf}
    \includegraphics[width=0.35\textwidth]{graphics/dune_neutrino_nd_numubar.pdf}
     \includegraphics[width=0.35\textwidth]{graphics/dune_antineutrino_nd_numubar.pdf}
    \includegraphics[width=0.35\textwidth]{graphics/dune_neutrino_nd_nue.pdf}
     \includegraphics[width=0.35\textwidth]{graphics/dune_antineutrino_nd_nue.pdf}
    \includegraphics[width=0.35\textwidth]{graphics/dune_neutrino_nd_nuebar.pdf}
     \includegraphics[width=0.35\textwidth]{graphics/dune_antineutrino_nd_nuebar.pdf}
\end{dunefigure}

%\todo{Add numbers for flavor composition of beam}
%zp: added fractions for optimized beam at FD
%zp: updated fig:flux_flavor to Optimized 
The predicted neutrino fluxes for neutrino and antineutrino mode configurations of LBNF are shown in Figure~\ref{fig:flux_flavor}.  In neutrino (antineutrino) mode, the beams are 92\% (90.4\%) muon neutrinos (antineutrinos), with wrong-sign contamination making up 7\% (8.6\%) and electron neutrino and antineutrino backgrounds 1\% (1\%).  Although %there is also expected to be 
we expect a small non-zero intrinsic tau neutrino flux, this is not simulated by G4LBNF.  Nor are neutrinos arising from hadron decay at rest. % are also not simulated.  

\begin{dunefigure}[Flux uncertainties at the \dword{fd} as a function of neutrino energy]{fig:flux_uncertainties_flavor}
{Flux uncertainties at the far detector as a function of neutrino energy in neutrino mode (left) and antineutrino mode (right) for, from top to bottom, muon neutrinos, muon antineutrinos, electron neutrinos and electron antineutrinos.   This is for the NDTF optimized flux; it will  be updated with latest beam design.   }
    \includegraphics[width=0.4\textwidth]{error_overlay_numu_neutrino_FD_opt.png}
    \includegraphics[width=0.4\textwidth]{error_overlay_numu_antineutrino_FD_opt.png}
    \includegraphics[width=0.4\textwidth]{error_overlay_numubar_neutrino_FD_opt.png}
    \includegraphics[width=0.4\textwidth]{error_overlay_numubar_antineutrino_FD_opt.png}
        \includegraphics[width=0.4\textwidth]{error_overlay_nue_neutrino_FD_opt.png}
    \includegraphics[width=0.4\textwidth]{error_overlay_nue_antineutrino_FD_opt.png}
        \includegraphics[width=0.4\textwidth]{error_overlay_nuebar_neutrino_FD_opt.png}
    \includegraphics[width=0.4\textwidth]{error_overlay_nuebar_antineutrino_FD_opt.png}
    \end{dunefigure}

\fixme{defin NDTF, PPFX. anne}


\begin{dunefigure}[Focusing and hadron production uncertainties on the $\nu$ mode $\nu_{\mu}$ flux]{fig:flux_uncertainty_breakdown}
{Focusing (left) and hadron production (right) uncertainties on the neutrino mode muon neutrino flux at the \dword{fd}.  This is for an older beam design and will be updated to the latest.  }
\includegraphics[width=0.45\textwidth]{focusing_error_overlay_numu_neutrino_FD_opt.png}
    \includegraphics[width=0.45\textwidth]{HP_error_overlay_numu_neutrino_FD_opt.png}\end{dunefigure}

Uncertainties on the neutrino fluxes arise primarily from uncertainties in hadrons produced off the target and uncertainties in parameters of the beam such as horn currents and horn and target positioning (commonly called ``focusing uncertainties'').  Uncertainties on the neutrino fluxes arising from both of these categories of sources are shown in Figure~\ref{fig:flux_uncertainties_flavor}.  Hadron production uncertainties are estimated using the PPFX framework developed by the \minerva collaboration~\cite{Aliaga:2016oaz, AliagaSoplin:2016shs}, which assigns uncertainties for each hadronic interaction leading to a neutrino in the beam simulation, with uncertainties taken from thin target data (from e.g., the NA49~\cite{NA49} experiment) where available, and large uncertainties assigned to interactions not covered by data.  Focusing uncertainties are assessed by altering beamline parameters in the simulation within their tolerances and observing the resulting change in predicted flux.  A breakdown of the hadron production and focusing uncertainties into various components are shown in Figure~\ref{fig:flux_uncertainty_breakdown} for the neutrino mode muon neutrino flux at the \dword{fd}.    

At most energies, hadron production uncertainties are dominated by the ``NucleonA'' category, which includes proton and neutron interactions that are not covered by external data.  At low energies, uncertainties due to pion reinteractions (denoted ``Meson Inc'') dominate.   The largest source of focusing uncertainty arises from a 1\% uncertainty in the horn current, followed by a 2\% flat uncertainty in the number of protons impinging on the target.   For all neutrino flavors and all neutrino energies, hadron production uncertainties are larger than focusing uncertainties.  However, hadron production uncertainties are expected to decrease in the next decade, as more thin target data becomes available.  Hadron production measurements taken with a replica target are also being considered and would substantially reduce the uncertainties.  


\begin{dunefigure}[Correlation of flux uncertainties]{fig:flux_uncertainty_correlation}
{Correlation of flux uncertainties.  Each block of neutrino flavor corresponds to bins of energy with bin boundaries of [0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 8.0, 10.0, 15.0, 20.0, 40.0, 100.0] for muon neutrinos and antineutrinos and of [0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 20.0, 100.0] for electron neutrinos and antineutrinos. }
    \includegraphics[width=0.9\textwidth]{correlation_ndtf_opt.png}\end{dunefigure}

Figure~\ref{fig:flux_uncertainty_correlation} shows correlations of the total flux uncertainties.  In general, the uncertainties are highly correlated across energy bins. However, the flux in the very high energy, coming predominantly from kaons, tends to be uncorrelated with flux at the peak, %dominated 
arising predominantly from pion decays.  Flux uncertainties are also highly correlated between the near and far detectors and between neutrino-mode and antineutrino-mode running.  The focusing uncertainties do not affect wrong-sign backgrounds, which reduces correlations between e.g., muon neutrinos and muon antineutrinos in the same running configuration in the energy bins where focusing uncertainties are significant.    

%Although the fluxes at the \dword{nd} and \dword{fd} are similar, they are not identical.  
The fluxes at the \dword{nd} and \dword{fd} are similar but not identical. Figure~\ref{fig:flux_nearfar} shows the ratio of the near and far neutrino-mode muon neutrino fluxes %at the near and far detectors 
and the uncertainties on the ratio.  The uncertainties are approximately 1\% or smaller except at the falling edge of the focusing peak, where they rise to 2\%, but are still much smaller than the uncertainty on the absolute fluxes.    And unlike the case for absolute fluxes, the uncertainty on the near-to-far flux ratio is dominated by focusing rather than hadron production uncertainties.  This ratio and its uncertainty are for the fluxes at the center of the detector, 
\fixme{which, near or far?}
and do not take into account small variations in flux across the face of the \dword{nd}.     

\begin{dunefigure}[Ratio of neutrino-mode muon neutrino fluxes at the near and far detectors]{fig:flux_nearfar}
{Ratio of neutrino-mode muon neutrino fluxes at the near and far detectors (left) and uncertainties on the ratio (right).  To be updated. }
    \includegraphics[width=0.45\textwidth]{nearfar.png}
     \includegraphics[width=0.45\textwidth]{nearfarunc.png}
\end{dunefigure}
\fixme{see the 'to be updated in figure}

\subsubsection{Off-axis Neutrino Flux and Uncertainties}

The neutrino flux spreads beyond the beam aimed directly at the \dword{fd}, and viable neutrino fluxes extend outward at the \dword{nd} hall.  For an ``off-axis'' angle relative to the initial beam direction, the subsequent neutrino energy spectrum is narrower and peaked at a lower energy than the on-axis spectrum. At $575\,\textrm{m}$, the location of the \dword{nd} hall, a lateral shift of $1\,\textrm{m}$ corresponds to approximately a $0.1^\circ$ change in off-axis angle.  Furthermore, the DUNE-PRISM concept, in which the near detector LArTPC %is movable 
can be moved to enable  off-axis measurements, relies on this feature to help constrain systematic errors for the \dword{lbl} oscillation program as described in Section~\ref{sec:ch-nu-osc-06-ndconcept-offaxis}. A discussion of off-axis flux modeling is given in %the Appendix, in 
Section~\ref{sec:tools-app-flx-offaxis}.



\subsubsection{Alternate Beamline Configurations}

Although the LBNF beamline is expected to run for many years in a \dword{cp}-optimized configuration, it could potentially be modified in the future for other physics goals.  For example, it could be altered to produce a higher energy spectrum %in order 
to measure tau neutrino appearance.  In the standard \dword{cp}-optimized configuration, we expect about 130 tau neutrino \dword{cc} interactions per year %are expected 
at the \dword{fd}, before detector efficiency and assuming 1.2 MW beam power.  However, replacing the three \dword{cp}-optimized horns with two NuMI-like parabolic horns can raise this number to approximately 1000 tau neutrinos per year.  Figure~\ref{fig:tau-optimized} shows the muon neutrino flux for one such configuration.  Although the flux in the 0-5 GeV region critical to $\delta_{CP}$ measurements is much smaller, the flux above 5 GeV, where the tau neutrino interaction cross section becomes significant, is much larger.  Many other energy distributions are possible by modifying the position of the targets and horns.  Even altering parameters of the \dword{cp}-optimized horns offers some variablity in energy spectrum, but the parabolic NuMI horns offer more configurability.  Because the LBNF horns are not expected to be remotely movable, such reconfigurations of the beamline would require lengthy downtimes to reconfigure target chase shielding and horn modules.   

\begin{dunefigure}[Comparison of standard and tau-optimized neutrino fluxes]{fig:tau-optimized}
{Comparison of standard and tau-optimized neutrino fluxes.  The tau optimized flux was simulated with a 120 GeV proton beam and two NuMI parabolic horns, with the second horn starting 17.5 downstream from the start of the first horn, and a 1.5 m long, 10 mm wide carbon fin target starting 2 m from the upstream face of the first horn.  To be updated.  }
%ZP: Updated plot (made legend bigger)
\includegraphics[width=0.5\textwidth]{graphics/tau_vs_nominal_neutrino_fd.pdf}
\end{dunefigure}

\fixme{figure To be updated}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Neutrino Interaction Generators} 
\label{sec:tools-mc-gen}

%\begin{itemize}
%\item Have this section owned by a Genie expert. Costas?
%\end{itemize}

%Both the beam neutrinos and atmospheric neutrinos were simulated using {\sc genie} v2\_10\_6. The beam neutrinos were simulated using the reference flux. We simulated unoscillated samples in both the neutrino and antineutrino modes. We also simulated fully oscillated samples where we convert all $\nu_\mu$'s in the beam to either $\nu_e$'s or $\nu_\tau$'s. 

%The atmospheric neutrinos were simulated using the Bartol flux for the Soudan site~\cite{ref:bartol}. There is a plan to use the Honda flux for the Homestake site~\cite{ref:honda} when the {\sc genie} flux interface is updated. 

\subsubsection{Supernova Neutrinos}

The \dword{snb} neutrino events were generated using custom code wrapped in a \dword{larsoft} module.
This code simulates \dword{cc} $\nu_e$-$^{40}$Ar interactions.
For each electron neutrino it calculates probabilities to produce a $^{40}$K nucleus
in different excited states (using a model from~\cite{Bhattacharya:1998hc}),
randomly selects one, and (with energy levels from~\cite{Cameron:2004myb}) 
produces several de-excitation $\gamma$s and an electron carrying the remaining energy.
All particles are produced isotropically, and 
there is no delay between the electron and corresponding de-excitation $\gamma$s
(in this model the $^{40}$K nucleus de-excites instantaneously) and they share a vertex,
which is simulated with equal probability anywhere in the active volume.
The primary neutrino energy distribution used in these samples is the cross-section-weighted 
energy spectrum obtained from SNOwGLoBES~\cite{snowglobes} (using the ``GKVM'' flux~\cite{GKVM}).
The \dword{snb} neutrino generator also allows to simulate a Poisson-distributed random number 
of neutrino interactions per event. These samples were simulated with, on average, 2 or 20 neutrinos.
In addition, one of the samples was generated with $1.01$~Bq/kg of $^{39}$Ar background.

\subsubsection{GENIE}

The DUNE \dword{mc} simulation chain is interfaced to the \dword{genie} event generator \cite{Andreopoulos:2009rq}. This is an open-source product of the \dword{genie} collaboration\footnote{www.genie-mc.org}  that provides state-of-the-art modeling of neutrino-nucleus interactions, as well as simulation of several other non-neutrino processes (nucleon decay, neutron-antineutron oscillation, boosted dark matter interactions, hadron and charged lepton scattering off nuclei). The generator product also includes off-the-shelf components (flux drivers and interfaces to outputs of detailed neutrino beamline simulations, detector geometry drivers, and several specialized event generation applications) for the simulation of realistic experimental setups. The \dword{genie} collaboration performs an advanced global analysis of neutrino scattering data, and is leading the development and characterization of comprehensive interaction models. The \dword{genie} comprehensive models and physics tunes which are developed using its proprietary Comparisons and Tuning products, are fully integrated in the \dword{genie} Generator product. Finally, the open-source \dword{genie} Reweight product provides means for propagating modeling uncertainties. 

At the time of the \dword{tdr} writing, the DUNE simulation uses a version in the v2 series of the \dword{genie} Generator, which includes empirical comprehensive models, based on home-grown hadronic simulations (AGKY model \cite{Yang:2009zx} for neutrino-induced hadronization and INTRANUKE/hA model \cite{Dytman:2015taa} for hadronic re-interactions) and nuclear neutrino cross sections calculated within the framework of the simple Relativistic Fermi Gas model \cite{Bodek:1981wr}. Several processes are simulated within that framework with the most important ones, in terms of the size of the corresponding cross section at few-GeV, being: (1) quasi-elastic scattering, simulated using an implementation of the Llewellyn Smith model \cite{LlewellynSmith:1971uhs}, (2) multi-nucleon interactions, simulated with an empirical model motivated by the Lightbody model \cite{Lightbody:1988gcu} and using a nucleon cluster model for the simulation of the hadronic system, (3) baryon resonance neutrino-production simulated using an implementation of the Rein-Sehgal model \cite{Rein:1980wg}, and (4) deep-inelastic scattering, simulated using the model of Bodek and Yang \cite{Bodek:2002ps}.  These comprehensive models, as well as the \dword{genie} procedure for tuning the cross section model in the transition region, have been used for several years and are well understood and documented \cite{Andreopoulos:2009rq}. The actual tune used is the one produced for the analysis of data from the MINOS experiment and, as it was already known at that time, it has several caveats as it emphasizes inclusive data and does not address tensions with exclusive data. The future DUNE simulation will be done using the v3 \dword{genie} Generator where improved models and tunes are available. %These comprehensive models and tunes are now unsupported, as they are superseded by the substantially improved versions listed below. However, they are included in v3 as they provide a connection with a large body of neutrino interaction studies.

%Building a comprehensive model for the simulation of neutrino interactions in the energy range of interest to current and near-future experiments poses significant challenges. This broad energy range bridges the perturbative and non-perturbative pictures of the nucleon and a variety of scattering mechanisms are important. In many areas, including elementary cross sections, hadronization models, and nuclear physics, one is required to piece together models with different ranges of validity in order to generate events over all of the available phase space. This inevitably introduces challenges in merging and tuning models, making sure that double counting and discontinuities are avoided. In addition, there are kinematic regimes which are outside the stated range of validity of all available models, in which case we are left with the challenge of developing our own models or deciding which model best extrapolates into this region. 
%
%At the time of writing this document, Generator v3.0.0 was the most recent version released by the GENIE Collaboration [(1) Paper on GENIE3 in progress -- Will provide reference ASAP]. This version includes several new, state-of-the-art comprehensive models and tunes, whose performance against a large collection of neutrino, charged-lepton and hadron scattering data was characterised in detail [(2) Paper with detailed model characterization in progress -- Will provide reference ASAP]. The following comprehensive models and tunes are currently available:
%
%{\bf G00\_00a\_00\_000} and {\bf G00\_00b\_00\_000}: They are equivalent to the historical ``Default'' and``Default+MEC'' models, as they were implemented in the latest releases of the v2 series of the GENIE Generator. They are empirical comprehensive models, based on home-grown hadronic simulations (AGKY model \cite{Yang:2009zx} for neutrino-induced hadronization and INTRANUKE/hA model \cite{Dytman:2015taa} for hadronic re-interactions) and nuclear neutrino cross-sections calculated within the framework of the simple Relativistic Fermi Gas model \cite{Bodek:1981wr}. Several processes are simulated within that framework with the most important ones, in terms of the size of the corresponding cross-section at few-GeV, being: a) quasi-elastic scattering, simulated using an implementation of the Llewellyn Smith model \cite{LlewellynSmith:1971uhs}, b) multi-nucleon interactions, simulated with an empirical model motivated by the Lightbody model \cite{Lightbody:1988gcu} and using a nucleon cluster model for the simulation of the hadronic system, c) baryon resonance neutrino-production simulated using an implementation of the Rein-Sehgal model \cite{Rein:1980wg}, and d) deep-inelastic scattering, simulated using the model of Bodek and Yang \cite{Bodek:2002ps}.  These comprehensive models, as well as the GENIE procedure for tuning the cross-section model in the transition region, have been used for several years and are well understood and documented \cite{Andreopoulos:2009rq}. The actual tune used is the one produced for the analysis of data from the MINOS experiment and, as it was already known at that time, it has several caveats as it emphasises inclusive data and does not address tensions with exclusive data. These comprehensive models and tunes are now unsupported, as they are superseded by the substantially improved versions listed below. However, they are included in v3 as they provide a connection with a large body of neutrino interaction studies.
%
%{\bf G18\_01a\_02\_11a} and {\bf G18\_01b\_02\_11a}: They are adiabatically improved versions of the historical comprehensive models. They include new processes (diffractive production of pions, hyperon production) and major upgrades to the final-state hadronic re-interaction models (the `01a' version uses the INTRANUKE/hA2018 hadronic re-interaction model, whereas `01b' uses the INTRANUKE/hN2018 model) [(1) Paper on GENIE3 in progress], but leave the base cross-section model unchanged with respect to the historical model. However, the cross-section model was re-tuned to deuterium data and shows much improved agreement with several exclusive $1\pi$ and $2\pi$ channels, while it maintained good agreement with inclusive data. [(3) Paper on tunes in progress -- Will provide reference ASAP]
%
%{\bf G18\_02a\_02\_11a} and {\bf G18\_02b\_02\_11a}: They are improved empirical models that share many of the features of G18\_01a\_02\_11a and G18\_01b\_02\_11a (new processes, improved hadronic simulations, new tunes), but introduced changes to the base cross-section model. In these comprehensive models, the Rein-Sehgal coherent and resonance neutrino-production models \cite{Rein:2006di, Rein:1980wg} were replaced with the better-motivated models produced by Berger-Sehgal \cite{Berger:2008xs, Berger:2007rq}.
%
%{\bf G18\_10a\_02\_11a} and {\bf G18\_10b\_02\_11a}: They are comprehensive models anchored on the best theory currently impleented in GENIE. They are similar to G18\_02a\_02\_11a and G18\_02b\_02\_11a, respectively, but for the simulation of quasielastic-like processes they implement a modern microscopic calculation by the Valencia group. Quasi-elastic processes, within a Local Fermi Gas (LFG) model and simulating Coulomb and Random Phase Approximation (RPA) corrections are implemented based on Ref. \cite{Nieves:2004wx}, whereas 2p2h processes are implemented based on Ref. \cite{Nieves:2011pp}.
%
%{\bf G18\_10i\_00\_000} and {\bf G18\_10j\_00\_000}: They are similar to G18\_10a\_02\_11a and G18\_10b\_02\_11a, but using the better-motivated $z$-expansion \cite{Hill:2010yb} of the axial form factor. However, due to extra complications in incorporating deuterium experiment flux constraints when using the $z$-expansion model in the GENIE tunes, both of these comprehensive models were untuned in v3.0.0, though a tune will be made available in a future revision. 

Besides simulation of neutrino-nucleus interactions, \dword{genie} provides simulation of several \dword{bsm} physics channels:

\textit{\dword{bsm}}: The implementation of a \dword{bsm} \dword{mc} simulation has been motivated by several theory studies \cite{Agashe:2014yua, Berger:2014sqa, Kong:2014mia, Cherry:2015oca, Kopp:2015bfa, Necib:2016aez, Alhazmi:2016qcs, Kim:2016zjx}. The current implementation focuses on two models presented in  \cite{Berger:2014sqa}. The first has a fermionic \dword{dm} candidate, a $Z^\prime$ mediator, and a $u^0$ velocity dependence of the spin-dependent cross section in the non-relativistic limit. The second model has a scalar \dword{dm} candidate, a $Z^\prime$ mediator, and a $u^2$ velocity dependence of the spin-dependent cross section in the non-relativistic limit.

\textit{Nucleon decay}: \dword{genie} simulates several nucleon decay topologies, using the exact same modeling of
\fixme{of or as?}
the initial nuclear state environment and intranuclear hadron transport as for the neutrino event simulation. In the nucleon decay simulation, the nucleon binding and momentum distribution is simulated using one of the nuclear models implemented in \dword{genie} (typically a Fermi Gas model), and it is decayed to one of many topologies using a phase space decay. The decay products are produced within the nucleus and further re-interactions of hadrons are simulated by the \dword{genie} hadron transport models. The simulated nucleon decay topologies are given in Table~\ref{tbl:genie_ndk}.

\textit{Neutron-antineutron oscillation}: \dword{genie} simulates several event topologies that may emerge following the annihilation of the antineutron produced from a bound neutron to antineutron transition. The simulation, as in the case of nucleon decay, uses the same modeling of 
\fixme{of or as?} the initial nuclear state environment and intranuclear hadron transport as for the neutrino event simulation. The simulated topologies are given in Table~\ref{tbl:genie_antineutron}.

\begin{table}
  \includegraphics[width=\linewidth]{costas_table1}
  
  \caption[\dword{genie} nucleon decay topologies]{Decay topologies considered in \dword{genie} nucleon decay simulation. TODO: replace raster version}
  \label{tbl:genie_ndk}
\end{table}

\begin{table}
	\begin{centering}
		\includegraphics[width=.8\linewidth]{costas_table2}\\
    \end{centering}
  
  \caption[\dword{genie} antineutron annihilation topologies]{Antineutron annihilation topologies considered in the \dword{genie} neutron-antineutron oscillation simulation. TODO: replace raster version}
  \label{tbl:genie_antineutron}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Detector Simulation}
\label{sec:tools-mc-detsim}

%\begin{itemize}
%\item Charged particle-argon interactions
%\item Radiologicals
%\begin{itemize} \item Table from Jason Stock \end{itemize}
%\item Geometries
%\begin{itemize} \item Figure showing ProtoDUNE, 1x2x6 \end{itemize}
%\item LArG4
%\item Photon Simulation
%\begin{itemize}
%\item LAr optical properties
%\item Photon Library (or replacement) method
%\item Photon library figure
%\end{itemize}
%\item TPC detector signal simulation (Xin et al.)
%\end{itemize}

\subsubsection{LArG4}\label{sec:larg4}

The truth particles generated in the event generator step are passed to a {\sc geant4} v4\_10\_1\_p03-based detector simulation. In this step, each primary particle from the generator and its decay or interaction daughter particles are tracked when they traverse \lar. The energy deposition is converted to ionization electrons and scintillation photons. Some electrons are recombined with the positive ions~\cite{Acciarri:2013met,Amoruso:2004dy} while the rest of the electrons are drifted towards the wire planes. The number of electrons is further reduced due to the existence of impurities in the \lar, which is commonly parameterized as the electron lifetime. The default electron lifetime is 3 ms in the simulation. The longitudinal diffusion smears the arrival time of the electrons at the wires and the transverse diffusion smears the electron location among neighboring wires. More details
regarding the recent measurements of diffusion coefficients can be found
in~\cite{Li:2015rqa,ref:lar_property}.

\subsubsection{Photon Simulation}

When ionization is calculated, the amount of scintillation light is also calculated. The response of the \dwords{pd} is simulated using a ``photon library,'' a pre-generated table giving the likelihood that photons produced within a voxel in the detector volume  will reach any of the \dwords{pd}. The photon library is generated using \dword{geant4}'s photon transport simulation, including \SI{66}{cm} scattering length, \SI{20}{m} attenuation length, and reflections off of the interior surface detectors. The library also incorporates the response versus location of the \dwords{pd}, capturing the attenuation between the initial conversion location of the photon and the \dwords{sipm}.

\subsubsection{TPC Detector Signal Simulation}\label{sec:tpc_sim}

When ionization electrons drift through the induction wire planes toward the collection wire plane, current is
induced on nearby wires. The principle of current induction is described by the Ramo theorem~\cite{Shockley1938,Ramo:1939vr}. 
For an element of ionization charge, the instantaneous induced current $i$ is proportional to the amount of drifted charge $q$: 
\begin{equation}\label{eq:shockley_ramo}
  i = - q \cdot \vec{E}_w \cdot \vec{v}_q.
\end{equation}
The proportionality factor is a product of the weighting field $\vec{E}_w$ at the location of the charge and 
the charge's drifting velocity $\vec{v}_q$. The weighting field $\vec{E}_w$ depends on the geometry of 
the electrodes. The charge's drifting velocity $\vec{v}_q$ is a function of the external \efield, which 
also depends on the geometry of the electrodes as well as the applied drifting and bias voltages. The current induced at a given electrode and electron drift path ($x$)
  sampled over a period of time ($t$) is called a ``field response function'' $R(x,t)$.

\begin{figure}[!htp]
\centering
\includegraphics[width=0.85\textwidth]{graphics/apa_cell.png}
\caption[Garfield configuration for simulating the field response functions]{Garfield configuration for simulating the field response functions.}
\label{field_resp_geometry}
\end{figure}

The field response functions for a single ionization electron are simulated with Garfield~\cite{garfield}. \fixme{add to glossary?} 
In the Garfield simulation, a region with 22 cm (along the \efield or drift direction) $\times$ 
30 cm (perpendicular to the field direction and wire orientation) is configured as shown in 
Figure~\ref{field_resp_geometry}. There are five wire planes with 4.71 mm spacing, referred to as G, U, V, X, and 
M with operating bias voltages of -665 V, -370 V, 0 V, 820 V, 0 V, respectively.  
These bias voltages ensure 100\% transmission of electrons through the grid plane (G) and the 
first two induction planes (U and V) and complete collection by the collection plane X with the main drift 
field at 500 V/cm. In the simulation, each wire plane contains 101 wires with 150 $\mu m$ diameter
  separated at $\sim$\SI{4.71}{\mm} wire pitch. The electron drift velocity as a function of electric
  field is taken from recent measurements~\cite{Li:2015rqa,ref:lar_property}. 

 \begin{figure}[!htp]
  \centering
  \includegraphics[width=0.7\textwidth]{graphics/field_response_data.png}
\caption[Position-dependent field response simulated with the Garfield program]{Position-dependent field response simulated with the Garfield program 
for two induction and one collection planes in the ``Log~10'' format. The wire of interest 
is assumed to locate at position zero. The ionization electron is assumed at different
positions in the transverse direction. See text for more discussion.}
\label{field_resp}
\end{figure}

 Given the above configuration, the field response function can then be calculated in Garfield
  for each individual wire  for an
  electron starting from any position within the region of simulation. 
  The field response functions for $\pm$10 wires on both sides of the central wire (21 wires in total) are 
  recorded. In addition to this position, five other positions with
  \SIlist{0.471;0.942;1.413;1.884;2.355}{\mm} horizontal shift towards one direction are also simulated.
  In total, 126 field responses (six positions for 21 readout wires) are calculated for each wire plane and stored for later
  application in the TPC detector signal simulation.
  Figure~\ref{field_resp} shows the simulated field response ($i$ in $\mu A$ with 0.1 $\mu s$ time bin) 
   in the ``Log~10'' format:
   \begin{equation}\label{eq:log10}
    i \text{ in ``Log 10'' } =
    \begin{cases}
      \text{log}_{10}(i\cdot4\times10^{10}),& \text{if }i>2.5\times10^{-11},\\
      0,& \text{if }  -2.5 \times10^{-11} \leq i \leq 2.5 \times 10^{-11},\\
      -\text{log}_{10}(- i\cdot4\times10^{10})& \text{if }i<-2.5\times10^{-11}.\\
      \end{cases}
  \end{equation}
  
  Following the earlier work in MicroBooNE~\cite{Adams:2018dra}, the TPC detector signal simulation
  is implemented in the software package Wire-Cell Toolkit~\cite{ref:wire_cell_toolkit,ref:full_simulation}, which is 
  further interfaced with \dword{larsoft}. This simulation procedure has been validated in the MicroBooNE experiment~\cite{Adams:2018gbi}. In
  the following, we summarize the major features. The TPC signal simulation takes input from the \dword{geant4}-simulated energy deposition when particles traverse the detector, and outputs digitized waveforms on the \dword{fe} electronics.
   A data-driven, analytical simulation of the inherent electronics noise is also performed. 
Figure~\ref{fig:simevent_wf} shows the example waveform for minimum ionizing particles traveling parallel to 
the wire plane, but perpendicular to the wire orientation.  Figure~\ref{fig:simevent} shows one example event of 
a \dword{mip} traveling through one \dword{apa}. 

\begin{figure}[htbp]
  \centering
        \includegraphics[width=0.6\textwidth]{graphics/DUNE_line_source_wf.png}
        \caption{Waveform for minimum ionizing particles traveling parallel to the wire plane. For different
        wire plane, the corresponding track is assumed to travel perpendicular to the wire orientation. 
        }\label{fig:simevent_wf}
         \includegraphics[width=0.95\textwidth]{graphics/track_sim.png}
    \caption{Simulated waveform for a \dword{mip} traveling through one \dword{apa}. The discontinuity of the 
    tracks in the V plane is caused by the wrapped wires.}
  \label{fig:simevent}
\end{figure}

The signal simulation, i.e., the \dword{adc} waveform on a given channel, 
\begin{equation}
  \label{eq:sim-convolution}
    M = (Depo \otimes Drift \otimes Duct + Noise) \otimes Digit, 
\end{equation}
%\footnote{symbol $\otimes$: circular convolution, also known as cyclic convolution or periodic convolution, arises in the context of the discrete-time Fourier Transform (DTFT).}
is conceptually a convolution of five functions:
\begin{description}
\item[$Depo$] represents the initial distribution of the ionization electrons created by energy depositions in space and time as
discussed in Section~\ref{sec:larg4}.
\item[$Drift$] represents a function that transforms an initial charge cloud to a distribution of electrons arriving at the wires. Physical processes related to drifting, including attenuation due to impurities, diffusion and possible distortions to the nominal applied \efield, are applied in this function.
\item[$Duct$] is a family of functions, each is a convolution $F \otimes E$ of the field response functions $F$ associated with the sense wire and the element of drifting charge and the electronics response function $E$ corresponding to the shaping and amplification of the \dword{fe} electronics. More details can be found in Section~\ref{sec:tools-mc-daq}.
\item[$Digit$] models the digitization electronics according to a given sampling rate, resolution, and dynamic voltage range and baseline offset resulting in an \dword{adc} waveform. 
\item[$Noise$] simulates the inherent electronics noise by producing a voltage level waveform from a random sampling of a Rayleigh distributed amplitude spectrum and uniformly distributed phase spectrum.  The noise spectra used are from measurements with the \dword{pdsp} detector after software noise filters, which have excess (non-inherent) noise effects removed.
\end{description}
%
These functions are defined over broad ranges and with fine-grained resolution. The resolutions are set by the variability (sub millimeter) and extent (several centimeters) of the field response functions and the sampling rate of the digitizer (\SI{0.5}{\micro\second}). Their ranges are set by the size of the detector (several meters) and the length of time over which one exposure is digitized (several milliseconds).  


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{DAQ Simulations and Assumptions}
\label{sec:tools-mc-daq}

%\begin{itemize}
%\item TPC electronics and noise simulation (Xin et al.) 
%\begin{itemize} \item Signal simulation/processing figure \end{itemize}
%\item Photon detector electronics simulation
%\end{itemize}

  The electrons on each wire are converted into raw wire signal (\dword{adc} vs Time) by convolution with the field response and electronics response. The \dword{asic} electronics response was simulated with the BNL SPICE~\cite{spice} simulation.  For most samples, the \dword{asic} gain was set to 14 mV/fC and the shaping time was set to 2 $\mu$s. For the samples generated with 3 mm wire spacing, the \dword{asic} gain was set to 25 mV/fC. The noise level was set to around 2.5 \dword{adc} RMS,based on extrapolation 
  from MicroBooNE experiment~\cite{Acciarri:2017sde}. This value was further validated in \dword{pdsp}. % experiment.
  Figure~\ref{elec_resp} shows the expected electronics shaping functions.

\begin{figure}[!h!tbp]
\centering
\includegraphics[width=0.7\textwidth]{elec.png}
\caption[\dword{asic}'s electronics shaping functions]{\dword{asic}'s electronics shaping functions
are shown for four shaping time settings at 14 mV/fC gain.}
\label{elec_resp}
\end{figure}


The \dword{pd} electronics simulation separately generates waveform 
for each channel (\dword{sipm}) of a \dword{pd} that has been hit by photons.
Every detected photon %that has been detected 
appears as a single \phel{} pulse
(with the shape taken from~\cite{http://lss.fnal.gov/archive/2015/pub/fermilab-pub-15-488-nd-ppd.pdf:2015gov}) on a randomly selected channel
(belonging to the \dword{pd} in which the photon was registered).
Then dark noise (with the rate of $10$ Hz) and 
line noise (Gaussian noise with the RMS of $2.6$ \dword{adc} counts) are added.
Each photon (or a dark-noise pulse) has a probability of appearing
as $2$ \phel{}s on a waveform (the cross-talk probability is $16.5~\%$).
The final step of the digitization process is recording only fragments
of the full simulated waveform that have a signal in them.
This is accomplished by passing the waveform through a hit finder
described (See the event reconstruction) \fixme{to fix} 
and storing parts of the waveform corresponding to the hits found.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Event Reconstruction in the \dword{fd}}
\label{sec:tools-fdreco}

This section describes various reconstruction algorithms used to reconstruct events in the \dword{fd} TPC. A successful \lartpc reconstruction needs to deliver reconstructed tracks and showers, particle and event identification, particle momentum and event energy. The reconstruction starts with finding signals on each wire above a threshold and building ``hits'' out of each pulse. All the LArTPC \threed reconstruction algorithms share the same principle. The $x$ coordinate is determined by the drift time and the $y$ and $z$ coordinates are determined by the intersection of two wires on different planes with coincident hits. There are currently three different reconstruction approaches in the DUNE reconstruction package. The \twod$\rightarrow$\threed reconstruction approach starts with clustering together nearby hits on each plane, %together 
followed by the use of time information to match \twod clusters between different planes to form \threed tracks and showers. Examples of this approach include TrajCluster and Pandora.  The direct \threed approach reconstructs \threed points directly from hits and then proceeds to perform pattern recognition using those \threed points. Examples of this approach include SpacePointSolver and Wire Cell. The third approach uses a deep-learning technique, known as a convolutional neural network (CNN). There are several tools needed to complete the task of LArTPC reconstruction. These tools include track fitter (PMA or KalmanFilter), calorimetry, \dword{pid} and track momentum reconstruction using range for contained tracks or multiple Coulomb scattering for exiting tracks. In addition to the TPC reconstruction, the \dword{pd} reconstruction provides trigger and $t_0$ information for non-beam physics. \fixme{add various terms above to glossary?}

%\begin{itemize}
%\item TPC reconstruction methods
%\begin{itemize}
%\item Demo figure for each method below
%\item TPC signal processing (Xin et al.)
%\item Hit reconstruction
%\item Track/Shower CNN
%\item TrajCluster
%\item PMA
%\item Pandora (Lorena)
%\item Wirecell (Xin et al.)
%\item SpacePointSolver
%\end{itemize}
%\item TPC Reco performance
%\begin{itemize} 
%\item Track finding efficiency vs. position, angle
%\item Shower purity/completeness comparison figures 
%\end{itemize}
%\item Photon detector reconstruction methods
%\begin{itemize}
%\item Hit finding
%\item Flash finding
%\begin{itemize} \item SN, PDK photon `event displays' \end{itemize}
%\item Calorimetry
%\end{itemize}
%\item Photon Reco Performance
%\begin{itemize}
%\item Efficiency vs. energy/position figures
%\item Timing resolution figure
%\item Energy resolution figure
%\end{itemize}
%\end{itemize}

\subsection{TPC Signal Processing}\label{sec:tpc_sp}

The raw data are in the format of \dword{adc} counts as a function of TPC ticks (0.5 $\mu$s) on each channel. The signal has a 
unipolar shape for a collection wire and a bipolar shape for an induction wire. The first step 
in the reconstruction is to reconstruct the distribution of ionization electrons arriving at anode plane. 
This is achieved by passing the raw data through a deconvolution algorithm. In real detectors, excess 
noise may exist and require removal %need to be removed 
through a dedicated noise filter~\cite{Acciarri:2017sde}. 

The deconvolution technique was introduced to LArTPC signal processing in the context ArgoNeuT 
data analysis~\cite{Baller:2017ugz}. The goal of the deconvolution is to ``remove'' the impact of field and 
electronics responses from the measured signal along the time dimension in order to reconstruct the number of ionized
electrons. This technique has the advantages of being robust and fast, and is an essential step in the overall drifted-charge profiling process. This 1D deconvolution procedure was improved to a \twod deconvolution procedure 
by the MicroBooNE collaboration~\cite{Adams:2018dra,Adams:2018gbi}, which further took into account the long-range 
induction effects in the spacial dimension. Two-dimensional software filters (channel and time) are implemented to 
suppress high-frequency noise after deconvolution procedure. For induction plane signals, regions of interest 
(ROIs) are selected to minimize the impact of electronics noise. More details of this algorithm can be found in~\cite{Adams:2018dra}.

This procedure, implemented in the Wire-Cell Toolkit software package~\cite{ref:wire_cell_toolkit},  has been used in the TPC signal processing in \dword{pdsp}. Figure~\ref{pDUNE_sp_wf}
shows an example induction U-plane waveform before and after the signal processing procedure. The bipolar 
shape is converted into a unipolar shape after the \twod deconvolution. Figure~\ref{pDUNE_sp_example}
shows full \twod image of induction U-plane signal from a \dword{pdsp} 
event~\cite{ref:pdune_signal_processing}. The measured signal (left) has a bipolar shape with red (blue) color 
representing positive (negative) signals. The deconvolved signal after 
the \twod deconvolution procedure (right) represents the reconstructed 
distribution of ionization electrons arriving at anode wire plane. The 
deconvolved signal becomes unipolar, and the long-range 
induction effect embedded in the field response is largely removed. 


\begin{figure}[!h!tbp]
\centering
\includegraphics[width=0.95\textwidth]{graphics/sample_5141_23865.pdf}
\caption[Measured and deconvolved waveform from an induction U-plane channel of \dword{pdsp}]{An example of measured (black) and deconvolved waveform from an induction U-plane channel of \dword{pdsp}
before and after the signal processing procedure. For measured waveform, the unit is ADC. For deconvolved
waveform, the unit is number of electrons after scaling down by a factor of 125.
Bipolar signal shape is converted into a unipolar signal shape after \twod deconvolution.}
\label{pDUNE_sp_wf}
\includegraphics[width=0.49\textwidth]{graphics/protodune_raw_u.pdf}
\includegraphics[width=0.49\textwidth]{graphics/protodune_decon_u.pdf}
\caption{Comparison of raw (left) and deconvolved induction U-plane signals (right) before and after 
the signal processing procedure from a \dword{pdsp} event. The bipolar shape with red (blue) color representing
positive (negative) signals is converted to the unipolar shape after the 2-D deconvolution. }
\label{pDUNE_sp_example}
\end{figure}


\subsection{Hit/Space-Point Identification}

\subsubsection{Gaussian Hit Finder}
The reconstruction algorithms currently employed by \larsoft are based on finding hits on the deconvolved waveforms for each plane. A key assumption is that the process of deconvolution will primarily result in gaussian shaped charged deposits on the waveforms and this drives the design of the aptly named ``gaushit'' 
\fixme{really? gaushit?} finder module. Generally, the module loops over the input deconvolved waveforms and handles each in three main steps: first it searches the waveforms %are searched 
for candidate pulses, it then fits these candidates %are then fit 
to a gaussian shape and, finally, it places the resulting hit %is then placed 
in the output hit collection. Not all charge deposits will be strictly gaussian shaped, for example a track can emit a delta ray and it can take several wire spacings before the two charge depositions are %will be 
completely separated. Alternatively, a track can have a trajectory at large angles to the sense wire plane creating a charge deposition over a large number of waveform ticks. The candidate peak-finding stage of the hit finder attempts to resolve the individual hits in both of these cases, still under an assumption that the shape of each individual charge deposition is %will be 
gaussian. If this results in candidate peak trains that are ``too long'' then special handling  breaks these into a number of evenly spaced hits and bypasses the hit-fitting stage. 

Figure~\ref{pDUNE_sp_hits} displays the results of the gaussian hit finder for the case of two or three hits only barely separated in \dword{pdsp} data. In this figure the deconvolved waveform is shown in blue, the red line represents the fit of the candidate peak to two or three gaussian shapes, the crosses represent the centers of the fit peaks, the pulse heights above the waveform baseline and their fit widths. 
\begin{figure}[!h!tbp]
\centering
\includegraphics[width=\textwidth]{graphics/hits.png}
\caption[An example of reconstructed hits in \dword{pdsp} data]{An example of reconstructed hits in \dword{pdsp} data. The deconvolved waveform is shown in blue, the red line represents the fit of the candidate peak to two or three gaussian shapes, the crosses represent the centers of the fit peaks, the pulse heights above the waveform baseline and their fit widths.
}
\label{pDUNE_sp_hits}
\end{figure}

\subsubsection{Space Point Solver}

\begin{figure}
\includegraphics[width=.5\linewidth]{graphics/evd_zy_pre_2302}
\includegraphics[width=.5\linewidth]{graphics/evd_zy_noreg_2302}\\
\makebox[.5\linewidth][c]{(a) All coincidences}
\makebox[.5\linewidth][c]{(b) Without regularization}\\
\includegraphics[width=.5\linewidth]{graphics/evd_zy_2302}
\includegraphics[width=.5\linewidth]{graphics/evd_zy_true_2302}
\makebox[.5\linewidth][c]{(c) With regularization}
\makebox[.5\linewidth][c]{(d) True charge distribution}\\

\caption[Event displays of SpacePointSolver performance]{Performance of SpacePointSolver on a simulated \dword{fd} neutrino interaction. The first panel shows the position of all triplet coincidences in the $zy$ view (looking from the side of the detector), displaying multiple ambiguous regions. The second and third panels show the solution with and without regularization, the regularization disfavouring various erroneous scattered hits. The final panel shows the true charge distribution, demonstrating %that 
the fidelity of the regularized reconstruction.}

\label{fig:spacepoint}
\end{figure}

The SpacePointSolver algorithm aims to transform the three \twod views provided by the wire planes into a single collection of \threed ``space points''.

First, triplets of wires are found with hits that are coincident in time within a small window (corresponding to \SI{2}{mm} in the drift direction) and where the crossing positions of the wires are consistent within \SI{3.55}{mm}. In some cases a collection wire hit may have only a single candidate pair of induction hits and the space point can be formed immediately. Often though, there are multiple candidate triplets, for example when two tracks are overlapped as seen in one view.

SpacePointSolver resolves these ambiguities by distributing the charge from each collection wire hit between the candidate space points so as to minimize the deviations between the expected and observed charges of the induction wire hits

\begin{equation}
\chi^2 = \sum_i^{\rm wires} \left(q_i - \sum_j^{\rm points}T_{ij}p_j\right)^2
\end{equation}

where $q_i$ is the charge observed in the $i^{\rm th}$ induction hit, $p_j$ is the solved charge at space point $j$, and $T_{ij}\in\{0,1\}$ encodes whether space point $j$ is coincident in space and time with wire hit $i$.

The minimization is subject to the condition that each predicted charge $p_j\ge0$, and that the total predicted charge for each collection wire hit exactly matches observations:

\begin{equation}
\sum_j^{\rm points}U_{jk}p_j=Q_k
\end{equation}

where $Q_k$ is the charge observed on the $k^{\rm th}$ collection wire, and $U_{jk}$ encodes the coincides of space point charges with the collection wires.

The problem as formulated is convex and can thus be solved exactly in a deterministic fashion. A single extra term can be added to the expression while retaining this property:

\begin{equation}
\chi^2 \to \chi^2 - \sum_{ij}^{\rm points}V_{ij}p_ip_j .
\end{equation}

By setting $V_{ij}$ larger for neighboring points this term acts as a regularization such that solutions with a denser collection of space points are preferred. The $V$ function is chosen empirically to have an exponential fall-off with constant \SI{2}{cm}m. 
\fixme{fix unit}
%The algorithm is described more completely in \cite{spacepoint}.

Figure \ref{fig:spacepoint} shows the performance of this algorithm on a sample \dword{fd} \dword{mc} event, demonstrating good performance at eliminating spurious coincidences, and the importance of the regularization term.

SpacePointSolver was developed with the intention of acting as the first stage of a fully \threed reconstruction for \dword{fd} neutrinos, but it has been successfully put to use in a more restricted role to solve the disambiguation problem in ProtoDUNE. The full problem is solved, but for this application the information retained is restricted to the drift volume to which the corresponding space points for each induction hit are assigned. This technique correctly resolves more than 99\% of hits while requiring less CPU time than the standard disambiguation algorithm.

\subsubsection{Disambiguation}
The induction wires are wrapped in the \dword{fd} TPC design in order to save cost on electronics and minimize dead region between \dword{apa}s. The consequence is that multiple induction wire segments will be read out by the same electronic channel. We need to determine which wire segment corresponds to the energy deposited by the particle in the TPC. This process is called disambiguation. 

The \dword{fd} disambiguation algorithm was originally developed for the \dword{35t} geometry. It relies on the fact that the collection wires are not wrapped and the wire angles are slightly different for the two inductions views (44.3$^{\circ}$ versus 45.7$^{\circ}$) in the \dword{35t} so that any three wires from the three planes read out by the same three channels will never cross twice. The algorithm uses gaushit as input. All the collection hits are unambiguous. Each induction hit has one channel ID and several possible wire IDs corresponding to various wire segments read out by the same channel. We need to determine the correct wire ID % which wire ID is the correct one 
for each induction hit. The disambiguation algorithm first loops over all collection hits. For each collection hit, it loops over all induction hits and looks for one hit on each of the two induction planes that are in time with the collection hit. Once the triplet of hits is found (one on each plane with a common time), the algorithm checks all possible wire IDs and looks for three wires that intersect. Once one and only one intersection is identified, the two induction wire IDs are assigned to the two induction hits and the ambiguation is resolved. Finally the algorithm loops over all unresolved induction hits. For each hit, it loops over all possible wire segments for that channel and chooses the wire segment that is closest to a resolved induction hit as the correct wire segment. 

The ProtoDUNE disambiguation algorithm uses the outcome of the SpacePointSolver reconstruction, which associate a \threed point with three hits on three wire planes. The \threed point can be projected onto the induction plane and the ambiguity of the induction wire signal can be resolved by choosing the wire segment that is closest to the \threed point projection. 
%(RS: protoDUNE's APAs should see signals from single TPC only)

%\subsection{Blurred Cluster}\label{sec:BlurredCluster}
%The Blurred Cluster reconstruction method aims to construct two-dimensional shower-like clusters from deposits left in the detector by showers.  It specialises in shower reconstruction, especially in the separation of nearby showers in the reconstruction of, e.g., $\pi^0$ decay.  The algorithm first applies a weighted \twod Gaussian smearing to the hit map in order to introduce `fake hits' and distribute the charge deposited in the detector more realistically.  This proceeds by convolution of a Gaussian kernel, uniquely applied for each hit given information such as rough directionality of the showering particles and the width of the reconstructed hits in time in order to introduce the most accurate blurring possible.  Clustering follows by grouping neighbouring hits within the blurred region before removing any artificial hits and forming output clusters from the remaining hit collections.  BlurredCluster uses disambiguated gaushit as input and the output clusters are in turn used as input to the EMShower algorithm (see Section \ref{sec:EMShower}).  More details and detailed discussion are available in Ref~\cite{ref:blurredcluster}.

\subsection{Hit Clustering, and Track and Shower Reconstruction}

\subsubsection{Line Cluster}\label{sec:LineCluster}
The intent of the Line Cluster algorithm is to construct \twod line-like clusters using local information. The algorithm was originally known as Cluster Crawler. The ``Crawler'' name is derived from the similarity of this technique to ``gliders'' in \twod cellular automata. The concept is to construct a short line-like ``seed'' cluster of proximate hits in an area of low hit density where hit proximity is a good indication that the hits are indeed associated with each other. Additional nearby hits are attached to the leading edge of the cluster if they are similar to the hits already attached to it. The conditions are that the impact parameter between a prospective hit and the cluster projection is similar to those previously added and the hit charge is similar as well. These conditions are moderated to include high charge hits that are produced by large $dE/dx$ fluctuations and the rapid increase in $dE/dx$ at the end of stopping tracks while rejecting large charge hits from $\delta$-rays.
Seed clusters are formed at one end of the hit collection so that crawling in only one direction is sufficient. Line Cluster uses disambiguated gaushits as input and produces a new set of refined hits. More details on the Line Cluster algorithm can be found in~\cite{ref:linecluster}.

\subsubsection{TrajCluster}\label{sec:TrajCluster}
TrajCluster reconstructs \twod trajectories in each plane. It incorporates elements of pattern recognition and Kalman Filter fitting. The concept is to construct a short ``seed'' trajectory of nearby hits. Additional nearby hits are attached to the leading edge of the trajectory if they are similar to the hits already attached to it. The similarity requirements use the impact parameter between the projected trajectory position and the prospective hit, the hit width and the hit charge. This process continues until a stopping condition is met such as lack of hits, an abnormally high or low charge environment, or encountering a \twod vertex or a Bragg peak.

\twod vertices are found between trajectories in each plane. The set of \twod vertices is matched between planes to create \threed vertices. A search is made of the ``incomplete'' \threed vertices, those that are only matched in two planes, to identify trajectories in the third plane that were poorly reconstructed.

Two recent additions to TrajCluster are matching trajectories in \threed and tagging of shower-like trajectories. More details on the TrajCluster algorithm can be found in~\cite{ref:trajcluster}.



\subsubsection{Pandora}\label{sec:Pandora}

The Pandora Software Development Kit~\cite{Marshall:2015rfa} was created to address the problem of identifying energy deposits from individual particles in fine-granularity detectors, using a multi-algorithm approach to solving pattern-recognition problems. Complex and varied topologies in particle interactions, especially with the level of detail provided by \lartpc{}s, are unlikely to be solved successfully by a single clustering algorithm. Instead, the Pandora approach is to break the pattern recognition into a large number of decoupled algorithms, where each algorithm addresses a specific task or targets a particular topology. The overall event is then built up carefully using a chain of many tens of algorithms. The Pandora multi-algorithm approach has already been applied to \lartpc{} detectors, and has been successfully used in different analyses for the automated reconstruction of cosmic ray muons and neutrino interactions in the MicroBooNE experiment~\cite{Acciarri:2017hat} as well as test beam interactions in the \dword{pdsp} detector (see Section~\ref{sec:Pandora:ProtoDUNE}).


%\subsubsection{Pandora Inputs and Outputs}

%The sole input to the Pandora pattern recognition is a list of reconstructed and disambiguated 2D Hits, along with detector information (such as dimensions, unresponsive or dead material regions, etc.). Via the {\it larpandora} repository, which integrates Pandora pattern recognition algorithms into the LArSoft framework, these hits are translated into native Pandora 2D hits and separated into the different views and into ``drift volumes'', defined as the regions of the detector with a common drift readout. The specified chain of pattern recognition algorithms is then initialised and applied. 

%The results of the pattern recognition are output into the ART/LArSoft framework, again via {\it larpandora}. The output of the reconstruction is illustrated in Figure~\ref{larsoft_output}, being the main output a list of reconstructed 3D particles (termed ``PFParticles'', where ``PF'' stands for Particle Flow) for each event. A PFParticle corresponds to a distinct track or shower in the event, and has associated collections (Clusters) of 2D hits for each view, as well as an associated set of reconstructed 3D positions (SpacePoints) and a reconstructed Vertex position that defines its interaction point or first energy deposit. Navigation along PFParticle hierarchies is achieved using the PFParticle interface, which connects parent and daughter PFParticles, providing the Particle Flow describing the interaction. In the reconstruction of neutrino interactions, a neutrino PFParticle is also created and forms the top-level parent particle in each reconstructed neutrino hierarchy, with only a Vertex associated to it. Pandora also provides a second level of ART associations linking elements such as Clusters, Slices, SpacePoints, Tracks and Showers with their corresponding collection of Hits. 

%Pandora uses the concept of {\it slice} internally to represent and separate subsets of hits topological distinct, based on distance and pointing information, with the aim of avoiding merging together into one hierarchy particles produced by the interaction of a neutrino or test beam particle and those with cosmic ray origin. An ART/LArSoft product is now also available to identify PFParticles and Hits that belong to the same {\it slice}. For PFParticles, this information is also stored in the Metadata object, which contains as well information handled internally by Pandora that might be helpful for usage downstream, namely whether a particle was identified as a clear cosmic, or the scores of the multivariate analysis tools, such as Boosted Decision Trees (BDTs) or Support Vector Machines (SVMs) that were used to identify the test team particle or the neutrino interaction respectively. 

%The identity of each particle is currently not reconstructed by Pandora, but PFParticles are instead characterised as track-like or shower-like using the topological information available at this stage, according to the pattern of their hits. Then, for each PFParticle a higher level object will be created, either a Track or a Shower depending on its classification, containing extra information such as direction, opening angle, etc.  

%\begin{figure}[!h!tbp]
%\centering
%\includegraphics[width=0.8\textwidth]{./FIG_LArPandoraOutputNew.png}
%\caption{Illustration of the Pandora output in ART/LArSoft. Navigation along PFParticle hierarchies is achieved using the PFParticle interface, represented by dashed lines. Navigation from PFParticles to their associated objects is represented by solid arrows, as well as the second level of ART associations from these objects to the underlying Hits input to Pandora.}
%\label{larsoft_output}
%\end{figure}

%\subsubsection{Overview of the Pandora Pattern Recognition Algorithms}

The input to the Pandora pattern recognition is a list of reconstructed and disambiguated \twod hits, alongside detector information (such as dimensions, unresponsive or dead material regions). On these input hits (once translated into native Pandora \twod hits), the specified chain of pattern-recognition algorithms is applied. The results of the pattern recognition are persisted in the \dword{art}/\dword{larsoft} framework, with the major output being a list of reconstructed \threed particles (termed ``PFParticles'', where ``PF'' stands for Particle Flow). A PFParticle corresponds to a distinct track or shower in the event, and has associated objects such as collections of \twod hits for each view (Clusters), \threed positions (SpacePoints) and a reconstructed Vertex position that defines its interaction point or first energy deposit. Navigation along PFParticle hierarchies is achieved using the PFParticle interface, which connects parent and daughter PFParticles, providing a Particle Flow description of the interaction. The identity of each particle is currently not reconstructed by Pandora, but PFParticles are instead characterized as track-like or shower-like based on their topological features. 

The main stages of the Pandora pattern recognition chain are outlined below, and illustrated in Figure~\ref{reco_steps}. Note that both the individual pattern recognition algorithms and the overall reconstruction strategy are under continual development and will evolve over time, with a current emphasis %being the 
on the inclusion of machine-learning approaches to drive decisions in some key algorithms. The current chain of pattern-recognition algorithms has largely been tuned for neutrino interactions from the \fnal Booster Neutrino Beam; however, the algorithms are designed to be generic and easily reusable, and they are in the process of being adapted for neutrino interactions in the energy regime of DUNE. A more detailed description of the algorithms can be found in~\cite{Acciarri:2017hat}. 


\begin{enumerate}
\item{\bf Input hits:} The input list of reconstructed and disambiguated \twod hits are translated into native Pandora \twod hits and separated into the different views and into ``drift volumes'', defined as the regions of the detector with a common drift readout.
\item{\bf \twod track-like clusters:}  The first phase of the Pandora pattern recognition is track-oriented \twod clustering, creating ``proto-clusters'' that represent continuous, unambiguous lines of \twod hits. This early clustering phase is careful to ensure that the proto-clusters have high purity (i.e., represent energy deposits from exactly one true particle) even if this means they are initially of low completeness (i.e., only contain a small fraction of the total hits within a single true particle). A series of cluster-merging and cluster-splitting algorithms then examine the \twod proto-clusters and try to extend them, making decisions based on topological information, aiming to improve completeness without compromising purity.
\item{\bf \threed vertex reconstruction:} The neutrino interaction vertex is an important feature point. Once identified, any \twod clusters can be split at the projected vertex position, reducing chances of merging particles in any view. Cluster-merging operations also take proximity to the vertex into account, in order to protect primary particles emerging from the vertex region, and ensure good reconstruction performance for interactions with many final-state particles. Pairs of \twod clusters from different views are first used to produce lists of possible \threed vertex positions. These candidate vertices are examined and scored, and the best vertex is selected. Pandora has developed different algorithms for the selection of the neutrino vertex, including the use of machine-learning approaches in MicroBooNE. Similar approaches can be harnessed in the future for interactions in %DUNE FD
the \dword{fd}, where a score-based approach is currently used.
\item{\bf \threed track reconstruction:} The aim of the \threed track reconstruction is to identify the combinations of \twod clusters (from the different views) that represent the same true, track-like particle. These \twod clusters are formally associated by the construction of a \threed track particle. During this process, \threed information can also be used to improve the quality of the \twod clustering. A Pandora algorithm considers all possible combinations of \twod clusters, one from each view, and builds (what is loosely termed) a rank-three tensor to store a comprehensive set of cluster-consistency information. This tensor can be queried to identify and understand any cluster-matching ambiguities. \threed track particles are first built for any unambiguous combinations of \twod clusters. Cases of cluster-matching ambiguities are then addressed, with iterative corrections to the \twod clustering being made to resolve the ambiguities and so enable \threed particle creation.
\item{\bf \twod and \threed shower reconstruction:} A series of topological metrics (additional use of some calorimetric information would be desirable in the future) are used to characterize each \twod cluster as track-like or shower-like. This information is analyzed to identify the longest shower-like clusters, which form the ``seeds'' or ``spines'' for \twod and \threed shower reconstruction. A recursive algorithm is used to add shower branches onto each top-level shower seed, then branches onto branches, etc. The \twod showers are then matched between views to form \threed showers, reusing ideas from the \threed track-matching procedure.
\item{\bf \twod and \threed particle refinement and event building:} Following the \threed track and shower reconstruction, a series of algorithms is used to improve the completeness of the reconstructed particles by merging together any nearby particles that are just fragments of the same true particle. Both \twod and \threed approaches are used, %with a typical approach being to 
where a typical approach uses combinations of \twod clusters (from different views) to identify features in \threed, or projecting \threed features into each of the \twod views. This is a powerful demonstration of the Pandora rotational coordinate transformation system, which allows seamless use of \twod and \threed information to drive pattern-recognition decisions. Finally, \threed space points are created for each \twod input hit, and the \threed particle trajectories are used to organize the reconstructed particles into a hierarchy. Final-state particles can be navigated via parent-daughter links, thus reconstructing their subsequent interactions or decays. For neutrino interactions, a top-level reconstructed neutrino particle is created; it represents the primary particle in the hierarchy linking together the daughter final-state particles and provides the information about the neutrino interaction vertex.

\end{enumerate}

\begin{figure}[!h!tbp]
\centering
\includegraphics[width=5cm, height=7.5cm]{./Pandora/OneInputHits.pdf}\includegraphics[width=13cm, height=7.5cm]{./Pandora/TwoPrePostTopAssoc.pdf}
\includegraphics[width=5cm, height=7.5cm]{./Pandora/ThreeVertexCandidates.pdf}\includegraphics[width=7cm, height=7.5cm]{./Pandora/Four3DMatch.pdf}\includegraphics[width=6cm, height=7.5cm]{./Pandora/FiveTrackShower.pdf}
\includegraphics[width=12cm, height=7.5cm]{./Pandora/SixPrePostMopUp.pdf}
\caption{Illustration of the main stages of the Pandora pattern recognition chain: (1) Input Hits; (2) \twod track-like cluster creation and association; (3) \threed vertex reconstruction; (4) \threed track reconstruction; (5) Track/Shower separation; (6) \twod and \threed particle refinement and event building.}
\label{reco_steps}
\end{figure}

The algorithms forming the stages described above can be used in different ways, thanks to the multi-algorithm approach. Currently, two Pandora reconstruction paths ({\it PandoraCosmic} and {\it PandoraNu}) have been created, using chains of tens of algorithms each (note that over 130 algorithms and tools are used in total). Although many algorithms are shared between the two paths, the overall algorithm selection results in different key features:
\begin{enumerate}
\item PandoraCosmic: Strongly track-oriented, optimized for the reconstruction of cosmic-ray muons and their daughter (shower-like) delta rays. 
\item PandoraNu: Optimized for the reconstruction of neutrino or test beam particle interactions, carefully building the event using the reconstructed interaction vertex (protecting particles emerging from it) and including a careful treatment of tracks vs showers. 
\end{enumerate}

These two chains of algorithms are harnessed together to provide a consolidated output in the case of surface detectors exposed to cosmic rays, such as MicroBooNE and \dword{pdsp} (without significant cosmic-ray background, only the PandoraNu algorithm chain is necessary for the \dword{fd}). The overall reconstruction strategy in such detectors is illustrated in Figure~\ref{consolidated_reco}. It starts by running the PandoraCosmic reconstruction on the entire collection of input hits, then identifies ``clear'' cosmic rays. This identification uses a geometrical approach to tag through-going cosmic rays and examines the consistency of the cosmic rays with the $t_{0}$ appropriate to the neutrino beam spill. Clear cosmic rays are output already at this stage. For the remaining ambiguous hits, however, additional stages are required. A \textit{slicing} process is applied to the remaining hits, dividing them into smaller regions (slices) that represent separate, distinct interactions. Each slice is reconstructed using both the PandoraNu and PandoraCosmic reconstruction chains and the results are compared directly to identify whether the slice corresponds to a cosmic ray or a neutrino interaction (in the case of MicroBooNE) or test beam interaction (in the case of \dword{pdsp}). The consolidated event output is formed of three classes of reconstructed particles: (1) clear cosmic rays, (2) cosmic rays that are spatially and temporally consistent with being a neutrino interaction in the detector (remaining cosmic-rays) and (3) candidate neutrino or test beam interactions.

\begin{figure}[!h!tbp]
\centering
\includegraphics[width=0.65\textwidth]{./FIG_PandoraConsolidatedOutput.png}
\caption[Schema of the Pandora consolidated output and overall reconstruction strategy for surface LArTPCs]{Schema of the Pandora consolidated output and overall reconstruction strategy for surface LArTPCs such as MicroBooNE and \dword{pdsp}. See text for more details.}
\label{consolidated_reco}
\end{figure}

Of particular importance in this overall reconstruction strategy is the neutrino (MicroBooNE) or test beam particle (\dword{pdsp}) identification tool. This tool is responsible for deciding whether to output the cosmic ray or neutrino (or test beam) reconstruction outcomes for a given slice. For \dword{pdsp}, this decision is based on the output from adaptive Boosted Decision Trees (BDTs), \fixme{gloss?} trained to distinguish between cosmic-ray and test beam particles, which has proved to be highly efficient across the momentum range of \dword{pdsp} data (as it will be shown in Section ~\ref{sec:Pandora:ProtoDUNE}). %The variables used in this BDT model have proved to be highly efficient across the momentum range of ProtoDUNE-SP data (as it will be shown in Section ~\ref{sec:Pandora:ProtoDUNE}). These variables are: 

%\begin{itemize}
%\item The eigenvalues of the covariance matrix of the spatial position of the 3D LArTPC hits.
%\item The distance of the closest 3D hit to the beam spot.
%\item The vertical distance of the reconstructed 3D hit closest to the top of the detector.  
%\item The number of reconstructed particles.
%\item The angle and direction of a spatial fit to the reconstructed 3D hits with respect to the beam line.
%\end{itemize}

The performance obtained with the current algorithms are shown in Section~\ref{sec:performance}, both for the \dword{fd} and \dword{pdsp}.  As previously mentioned, both the individual pattern recognition algorithms and the overall reconstruction strategy are under continual development. Many algorithms %are still to be explicitly tuned 
still require explicit tuning for the DUNE energy ranges, and new algorithms, designed specifically for DUNE, will be added to the multi-algorithm pattern recognition. The performance presented in this document therefore represents a current snapshot and is expected to improve with future dedicated work. 


\subsubsection{Projection Matching Algorithm}\label{sec:PMA}
Projection Matching Algorithm (PMA) \fixme{gloss} was primarily developed as a technique of \threed reconstruction of individual particle trajectories (trajectory fit) Ref~\cite{Antonello:2012hu}. PMA was designed to address a challenging issue of transformation from a set of independently reconstructed \twod projections of objects into a \threed representation. Reconstructed \threed objects are also providing  basic physics quantities like particle directions and $dE/dx$ evolution along the trajectories. PMA uses as its input the output from \twod pattern recognition: clusters of hits. For the purposes of the DUNE reconstruction chain the Line Cluster algorithm (Section~\ref{sec:LineCluster}) is used as input to PMA, however the use of hit clusters prepared with other algorithms may be configured as well. As a result of \twod pattern recognition, particles may be broken in \twod projections into several clusters, 
\fixme{broken into several clusters of 2d projections?}  fractions of particles may be missing in individual projections, and clusters obtained from complementary projections % are not guaranteed to 
may not cover corresponding sections of trajectories. Such behavior is expected since ambiguous configurations of trajectories can be resolved only if the information from multiple \twod projections is used. Searching for the best matching combinations of clusters from all \twod projections was introduced to PMA implementation in the \dword{larsoft} framework. The algorithm also attempts to correct hit-to-cluster assignments using properties of \threed reconstructed objects. In this sense PMA is also a pattern-recognition algorithm.

The underlying idea of PMA is to build and optimize objects in \threed space (formed as polygonal lines with % an iteratively increased number of segments) 
the number of segments iteratively increased) by minimizing the cost function calculated simultaneously in all available \twod projections. The cost function consists of the \twod distance of hits to the optimized object \twod projections, penalty of tracks curvature, and \threed distance of various feature points to the optimized object (used e.g., to improve performance for tracks with isochronous orientation).
\fixme{please clarify prev sentence. anne} The track can be reconstructed using clusters from two projections while the distance of hits to the track projection in the third plane is used to validate correct association of clusters. This method is used to score \threed track candidates in the three-plane TPC configurations, like the \dword{spmod}, %single-phase DUNE 
MicroBooNE, %detectors 
and prototypes. In this scenario clusters from all planes are used in the fine-tuning of the selected candidates. In the two-plane configurations (\dual TPC detectors, \single LArIAT and ArgoNeut  detectors) the track candidates are scored by the value of the cost function, which is significantly increased if the trajectory fit is being optimized to spuriously associated clusters
 \fixme{if we optimize the fit to do what with spuriously associated clusters? anne}(this is also a second-level criterion for the three-plane configurations %if 
 in cases where the validation method shows no significant difference for track candidates).


The approach of constructing entire objects in \threed %allows to avoid requirement of 
avoids the need to find associations between \twod planes on the level of individual hits. Such associations are especially problematic if several hits on a single trajectory can be found with a similar drift time value, or, if  due to a low signal, for example, % (or any other reason) 
hits are missed in one of projections. Unambiguous \threed position is calculated for each \twod hit, independently from other hits in the trajectory. Such \threed positions are more accurate than found with the ``standard'' calculation of a single \threed point using multiple \twod hits matched by drift time values, leading to the more accurate $dE/dx$ estimation. It also allows %to build 
construction of \threed objects using the detector data from \twod planes directly, without an intermediate step of %a \threed points calculation which are subsequently 
calculating \threed points for subsequent use in obtaining the final trajectory fit. Another advantage of the approach is the capability to estimate direction of small, few-hit tracks, %or estimation of 
and to estimate the \threed PC axis of shower-like objects. \fixme{what's PC? anne}

Several features were developed in \dword{larsoft}'s PMA implementation to address detector-specific issues like stitching the particle fragments found in different TPCs or %an option for 
performing disambiguation at the \threed reconstruction stage. Since algorithms existing within or interfaced to the \dword{larsoft} framework (see Section \ref{sec:Pandora}) can provide pattern reconstruction results that include the particle hierarchy description, the mode for applying PMA to calculate %solely the 
trajectory fits alone was developed. In this mode the collections of clusters forming particles are taken from the ``upstream'' algorithm and  hit-to-cluster associations remain unchanged. %are not changed.


%Recent developments of PMA are relying on the same principle ideas and allow to build and optimize complex structures of \threed objects, i.e. multiple particle trajectories interconnected with interaction vertices. With such approach it is possible to employ in the vertex position reconstruction the local information from several tracks simultaneously, leading also to an improved fit of each individual trajectory. The track-vertex structure is constructed after the individual trajectories are found and stitched across TPC's. Vertex candidates are found as regions of intersection of two or more tracks (including regions found beyond the track endpoints), with the threshold on the allowed region size. Tracks shorter than $nn$ cm are treated separately; they can be associated to vertex candidates found using long tracks in the first pass of the algorithm or used to create vertex candidates in the second pass. Candidates are scored by the number of intersecting tracks, and if this number is equal for two candidates the maximum angle between intersecting tracks is used select candidate and create a better defined vertex as first. The entire track-vertex structure with the newly created vertex is re-optimized using PMA principles in order to accommodate additional information in the trajectory fits. Since the new vertices may connect structures that already contain vertices, a set of rules for splitting and flipping tracks was developed to ensure that the resulting structure is always tree-like (i.e. does not contain loops). The resulting structure is described as a particle hierarchy using data products available in \dword{larsoft}.

\fixme{orig of next pgraph is commented above; I started rewording but I can't parse enough of it unambiguously. anne}
Recent PMA developments rely on the same principle and allow building and optimizing complex structures of \threed objects, e.g., multiple particle trajectories interconnected with interaction vertices. This makes it possible to use the local information from several tracks simultaneously in the vertex position reconstruction, which leads to an improved fit for each individual trajectory. 
%
PMA constructs the track-vertex structure after it finds the individual trajectories and stitches them across TPCs. Vertex candidates are found as regions of intersection of two or more tracks (including regions found beyond the track endpoints), with the threshold on the allowed region size. 
\fixme{what threshold? and fix nn cm below}
Tracks shorter than $nn$ cm are treated separately; they can be associated to vertex candidates found using long tracks in the first pass of the algorithm or \fixme{}used to create vertex candidates in the second pass. Candidates are scored by the number of intersecting tracks, and if this number is equal for two candidates the maximum angle between intersecting tracks is used select candidate and create a better defined vertex as first. The entire track-vertex structure with the newly created vertex is re-optimized using PMA principles in order to accommodate additional information in the trajectory fits. Since the new vertices may connect structures that already contain vertices, a set of rules for splitting and flipping tracks was developed to ensure that the resulting structure is always tree-like (i.e. does not contain loops). The resulting structure is described as a particle hierarchy using data products available in \dword{larsoft}.


Two important reconstruction features are currently under development in \dword{larsoft}'s PMA implementation. The first is the integration of PMA input with the recognition 
\fixme{integration with the `recognition'? Does that mean to add recognition capability to PMA? anne} of shower-like \twod objects (e.g., electromagnetic showers, electron tracks) and track-like objects (e.g., hadron and muon tracks). Such recognition is a prerequisite of obtaining robust \threed tracking with PMA, since different fitting strategies should be applied to track and shower objects. The second %feature under development 
is the detection of kinks and decay points missed at the \twod pattern recognition level. First attempts were made %with the search 
made by searching for outliers in the distribution of angles in the trajectory fit. It was found that the dependence on the track orientation in a \twod projection needs to be taken into account. In addition, a potential integration with an algorithm based on \twod \dword{adc} image analysis will be explored.


%\subsection{EMShower}\label{sec:EMShower}

%mode is used when reconstructing neutrino events.The EMShower reconstruction algorithm aims to find final 3D showers and all associated properties.  It is intentionally high-level by design and relies heavily on previous reconstruction, specifically BlurredCluster (Section \ref{sec:BlurredCluster}) and PMA (Section \ref{sec:PMA}).  The reconstruction proceeds in two general steps: first, the shower objects, including all associated hits in each of the views, are found; second, the properties of these showers, such as start point, direction, energy and initial dE/dx, are determined by multiple pattern recognition and calorimetric reconstruction algorithms.

%The shower objects are created by simply matching the previously found, well-formed, shower-like clusters (provided by BlurredCluster) between the different views to form 3D objects with associated hits in each plane.  This is achieved by associating hits between the \twod shower-like clusters and 3D tracks (provided by, e.g., PMA) in order to pull together clusters from different planes into one object.  These shower hits are then analysed by various successive algorithms in order to find relevant properties before the output shower objects are constructed for later use.

%EMShower can be configured to use hits from the shower-like PFParticles identified by the Pandora package. This mode is used when reconstructing neutrino events.

%Further details and detailed discussion can be found in Ref~\cite{ref:emshower}.


\subsection{Calorimetric Energy Reconstruction and Particle Identification}

As charged particles traverse a \lar volume, they deposit energy through ionization and scintillation. It is important to measure the energy deposition, as it provides information on particle energy and species. The algorithm for reconstructing the ionization energy in \dword{larsoft} is optimized for line-like tracks and is being extended to more complicated event topology such as showers. The algorithm takes all the hits associated with a reconstructed track and for %. For 
each hit, it converts the hit area or amplitude, in \dword{adc} counts, %is converted 
to the charge $Q_{det}$, in fC units, \fixme{whats fC? anne} on the wire using an \dword{adc}-to-fC conversion factor that was determined by muons or test stand measurements. To account for the charge loss along the drift due to impurities, a first correction is applied to $Q_{det}$ to get the free charge after recombination $Q_{free} = Q_{det}/e^{-t/\tau_{e}}$, where $t$ is the electron drift time for the hit and $\tau_{e}$ is the electron lifetime measured by the muons or purity monitors. The charge $Q_{\rm{free}}$ is divided by the track pitch $dx$, which is defined as wire spacing divided by the cosine of the angle between the track direction and the direction normal to the wire direction in the wire plane, to get the $dQ_{\rm{free}}/dx$ for the hit. Finally, to account for charge loss due to recombination, also known as ``charge quenching,'' a second correction is applied to convert $dQ_{\rm{free}}/dx$ to $dE/dx$ based on the modified Box's model~ \cite{Acciarri:2013met} or the Birks's model~\cite{Amoruso:2004dy}. The total energy deposition from the track is obtained by summing the $dE/dx$ from each hit: $\sum\limits_{i}^{all\ hits}(dE/dx)_{i}\cdot dx_{i}$.

If the incident particle stops in the LArTPC active volume, the energy loss $dE/dx$ as a function of the residual range ($R$), the path length to the endpoint of the track, is used as a powerful method for particle identification. There are two methods in \dword{larsoft} to determine particle species using calorimetric information. The first method calculates four $\chi^{2}$ values for each track by comparing measured $dE/dx$ %versus $R$ points to the proton, charged kaon, charged pion and muon hypotheses and identifies the track as the particle that gives the smallest $\chi^{2}$ value.
versus $R$ to hypotheses for the proton, charged kaon, charged pion and muon, and identifies the track as the particle that gives the smallest $\chi^{2}$ value. The second method calculates the quantity $PIDA = <A_{i}> = <(dE/dx)_{i}R_{i}^{0.42}>$ \cite{Acciarri:2013met}, which is defined to be the average of $A_{i} = (dE/dx)_{i}R_{i}^{0.42}$ over all track points where the residual range $R_{i}$ is less than 30 cm. The particle species can be determined by making a selection on the $PIDA$ value. 

%Figure~\ref{dedx} shows the $dE/dx$ versus residual range and $PIDA$ distributions for reconstructed $K^{+}$, $\mu^{+}$ and $e^{+}$ tracks in proton decay events. The calorimetry information is very efficient in separating those three different particles.

%\begin{figure}[!ht]
%\subfloat[$K^{+}$]{\includegraphics[width=0.5\textwidth]{dedxktc.pdf}}
%\subfloat[$\mu^{+}$]{\includegraphics[width=0.5\textwidth]{dedxmutc.pdf}}\\
%\subfloat[$e^{+}$]{\includegraphics[width=0.5\textwidth]{dedxetc.pdf}}
%\subfloat[$PIDA$]{\includegraphics[width=0.5\textwidth]{pidakmue.pdf}}
%\caption{(a)(b)(c): $dE/dx$ as a function of the residual range for reconstructed $K^{+}$, $\mu^{+}$ and $e^{+}$ tracks in proton decay events ($p\rightarrow\bar{\nu}K^{+}$, $K^{+}\rightarrow\mu^{+}\rightarrow e^{+}$). The black curves are theoretical predictions. (d): $PIDA$ distributions of the reconstructed $K^{+}$, $\mu^{+}$ and $e^{+}$ tracks.}
%\label{dedx}
%\end{figure}

\subsection{Wire Cell}

\begin{figure}[!ht]
 \includegraphics[width=0.98\textwidth]{graphics/wire-cell-overview.png}
\caption[Overview of \dword{wirecell} reconstruction paradigm]{Overview of\dword{wirecell} reconstruction paradigm, taken from~\cite{ref:wc_talk}. 
See text for more details.}
\label{wire-cell-overview}
\end{figure}


\Dword{wirecell} is a new reconstruction package under development. The current 
status of this reconstruction paradigm is shown in Figure~\ref{wire-cell-overview}. The 
simulation of the induction signal in a LArTPC and the overall signal processing process,
which are general to all reconstruction methods, are described in Sections~\ref{sec:tpc_sim} 
and~\ref{sec:tpc_sp}, respectively. The subsequent reconstruction in \dword{wirecell} adopts 
a different approach from the aforementioned algorithms. Instead of directly performing pattern 
recognition on each of the \twod views (drift time versus wire number), \threed imaging of events
is obtained with time, geometry, and charge information. This step is independent from 
the event topologies, and the usage of the charge information takes advantage of a unique 
feature of the projection views, as each of the wire plane detects the same 
amount of the ionization electrons under transparency condition. The strong requirement of the time, geometry, and charge 
information provides a natural way to suppress electronic noise
 while combing 
 \fixme{combining?} with successful signal processing maintains high hit efficiency. Details of this step is described in~\cite{Qian:2018qbv}. The subsequent reconstruction involves the object clustering and
TPC and light matching, which has been crucial for selecting neutrino interactions in the 
MicroBooNE experiment~\cite{uboone_wc_note}. The current focus of the \dword{wirecell} algorithm 
development is on the trajectory and $dQ/dx$ fitting, which aims at enabling precision particle
identification in LArTPC. %Beside these, 
Development of \threed pattern recognition also needs
to be revisited before reaching a complete reconstruction chain. 


\begin{figure}[!ht]
\centering
 \includegraphics[width=0.85\textwidth]{graphics/bee_event.png}
\caption{This \threed display shows the full size of the \dword{pdsp} detector (white box) and 
the direction of the particle beam (yellow arrow). Particles from other sources (such as cosmic rays) 
can be seen throughout the white box, while the red box highlights the region of interest: 
in this case, an interaction resulting from the 7 GeV beam particle through the detector. 
The \threed points are obtained using the Space~Point~Solver reconstruction algorithm. This event
can be accessed through interactive web-based event display Bee at \url{https://www.phy.bnl.gov/twister/bee/set/protodune-gallery/event/0/}.}
\label{wire-cell-bee}
\end{figure}

\dword{wirecell} team also created an advanced web-based \threed event display, ``Bee''~\cite{wire-cell-bee}, to aid the reconstruction development and provide interactive visualizations to end users.  Bee, together with \twod Magnify event 
display tools, have played important roles in 
the development of various reconstruction algorithms, including signal processing, \threed event 
imaging, object clustering, TPC and light matching, and trajectory and $dQ/dx$ fitting. The Bee event display 
was also used during the ProtoDUNE data-taking period to stream real-time reconstructed events to the users.
Figure~\ref{wire-cell-bee} shows an example of a data event from the \dword{pdsp} detector~\cite{ref:wc_bee}. 
The full video of this event can be found in~\cite{ref:bee_video}.

The primary software package hosting various \dword{wirecell} reconstruction algorithm is the 
\dword{wirecell} Toolkit (WCT)~\cite{ref:wire_cell_toolkit}, 
\fixme{gloss?}which provides a consistent, 
full-featured configuration system based on the Jsonnet \cite{jsonnet} data templating 
language and its C++ bindings.  Individual component construction and the application of
configuration data are handled by the toolkit, which allows for dynamic
construction patterns.  WCT supports a number of component execution models and in particular
provides one powerful built-in  model that follows the data flow
programming paradigm.  It connects toolkit components as nodes in a
directed graph that exchange data along the graph edges.  A flow graph
may be constructed in C++ or more simply via user configuration. The execution of a WCT 
flow graph is itself an abstracted component.  Two implementations currently exist.
One is single-threaded and minimizes memory usage.  The
second, based on Intel TBB~\cite{tbb} allows for multi-threaded
concurrent execution of components and exchanges data across graph edges
%which are 
implemented as thread-safe queues. The development of WCT follows a toolkit approach
and is designed to be easily integrated into larger applications. The toolkit makes strong use of 
abstractions for all public data objects and code units. With a shared library 
plugin system, the abstract interface implementation is made available to the application
using the toolkit. Currently, WCT has been integrated into the \dword{art} framework via a
package in the \dword{larsoft} software suite.  It provides an \dword{art} tool as the primary 
interface to the WCT as well as a number of WCT components which convert between \dword{larsoft} 
and WCT data representations. This interface is in regular use to run both MicroBooNE and 
\dword{pdsp} WCT jobs.



\subsection{Deep Learning}\label{sec:deeplearning}
Deep learning methods are used %There are 
in two main areas of the DUNE event reconstruction. % where . 
Both of these algorithms are based on convolutional neural networks (CNNs). \fixme{gloss}
In recent years CNNs have become the method of choice for many image recognition tasks in commerce and industry, and lately have been applied to high energy physics. The CNNs contain a series of filters that are applied to the input detector data images in order to extract the features required to classify the images.

\subsubsection{CNN for track and shower separation}
The hit-level CNN aims to classify each reconstructed hit as either track-like or shower-like by looking at the local region surrounding the hit in (charge, time) coordinates.  The CNN is trained using a large number of simulated images with the known true origin of the energy deposits. Once trained, the CNN provides the track-like or shower-like classification for each hit object in the event. This algorithm is applied to each readout view in each TPC separately. %The output from the algorithm is shown in Figure \ref{fig:trkshw_CNN_protodune} for a beam interaction in ProtoDUNE-SP. 

\subsubsection{CNN for event selection}
The algorithm used for the classification of neutrino interaction types is called the convolutional visual network (CVN) \fixme{gloss} and is  based on a CNN. The primary goal of the CVN is to provide a probability for each neutrino interaction to be \dword{cc}$\,\nu_\mu$, \dword{cc}$\,\nu_e$, \dword{cc}$\,\nu_\tau$ or \dword{nc}. The CVN takes three 500 $\times$ 500 pixel images of the neutrino interactions as input, one from each view. The images contain the charge and the peak time of the reconstructed hits and does not use any information beyond the hit reconstruction. The CVN is discussed in more detail in the TDR Physics Volume. \fixme{this is part of the physics volume...?}

\subsection{Optical Reconstruction}

\subsubsection{Optical Hit Finder}
\label{sec:OpticalHitFinder}
The first step of the DUNE optical reconstruction is reading
individual waveforms from the simulated \dword{pd} electronics
and finding optical hits -- regions of the waveforms containing pulses.
The optical hit contains the optical channel (\dword{sipm}) that the hit
was found on, time corresponding to the hit, its width,
area, amplitude, and number of \phel{}s.

The current DUNE optical-hit-finder algorithm searches for regions of the waveform
exceeding a certain threshold ($13$ \dword{adc} counts), checking whether that region
is wider than $10$ optical TDC ticks, \fixme{tdc?} and, if it is, calculating the aforementioned
optical-hit parameters for the region (including parts of the waveform around it
that have \dword{adc} values greater than $1$) and recording it as an optical hit.
The number of \phel{}s is calculated by dividing the full area of the hit
by the area of a single-\phel{} pulse.
The pedestal is assumed to be constant and is specified in the hit finder as $1500$ \dword{adc} counts.


\subsubsection{Optical Flash Finder}
After optical hits are reconstructed, they are grouped into higher-level objects called optical flashes.
The optical flash contains the time and time width of the flash,
its approximate $y$ and $z$ coordinates (and spatial widths along those axes),
its location and size in the wire planes,
the distribution of \phel{}s across all \dwords{pd},
and the total number of \phel{}s in the flash, among other parameters.

The flash-finding algorithm searches for an increase in \dword{pd} activity
(the number of \phel{}s) in time using information from optical hits
on all photon detectors.
When a collection of hits with the total number of \phel{}s  
greater than or equal to $2$ is found, the algorithm begins creating an optical flash.
It starts with the largest hit and adds hits from the found hit collection 
that lie closer than $0.5$ \fixme{units?} of the combined widths of the flash under construction
and the hit being added to it.
The flash is stored after no more hits can be added to it
and if it has more than $2$ \phel{}s.

The algorithm also estimates spatial parameters of the optical flash
by calculating the number-of-photoelectron-weighted mean and 
root mean square of locations of the optical hits
(defined as centers of \dwords{pd} where those hits were detected)
contained in the flash.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Reconstruction Performance}
\label{sec:performance}

An automated reconstruction of the neutrino interaction events in DUNE, often complex topologies with multiple final state particles, is a significant challenge. The currently used chain of Pandora pattern recognition algorithms %used 
has been tuned for neutrino interactions from the \fnal Booster Neutrino Beam, and it is in the process of being adapted for the wide range of energies of the DUNE \dword{fd}. Despite this fact, and thanks to the reusability of Pandora algorithms for different single phase \lartpc detectors,  good performance is already achieved with this first-pass pattern recognition. Significant improvements are expected  in the upcoming years with a more dedicated tune of the current algorithms, and the development of new ones, as needed. %according to the necessities of DUNE. 

%The following pages show figures assessing the performance of the current reconstruction. They are the result of the full reconstruction chain on the Monte Carlo simulations described in Section~\ref{sec:tools-mc}: i.e., signal processing, hit finder, disambiguation and Pandora pattern recognition. 
The current reconstruction performance, evaluated using metrics introduced in \ref{sec:Pandora:assessment}, is presented for simulated neutrino interactions in a \single \nominalmodsize \dword{fd} module in \ref{sec:Pandora:DUNEFD}, and for simulated and real data test beam events in \dword{pdsp} in \ref{sec:Pandora:ProtoDUNE}. These results outline the baseline performance on which improvements will continue to be made in the next years. In addition, current results on high-level reconstruction performance are presented in~\ref{sec:Pandora:High}.

\subsection{Pandora Performance Assessment}
\label{sec:Pandora:assessment}
The performance of the Pandora pattern recognition is assessed by matching reconstructed PFParticles to the simulated \dword{mc} particles (MCParticles). These matches are used to evaluate the efficiency with which MCParticles are reconstructed as PFParticles, and to calculate the completeness and purity of each reconstructed PFParticle. 

\fixme{mc and pfparticles in gloss?}

The following procedure is used to match reconstructed PFParticles with simulated MCParticles:

\begin{itemize}
\item \textit{Selection of MCParticles:} The full hierarchy of true particles is extracted from the simulated neutrino interaction. A list of ``target'' particles is then compiled by navigating through this hierarchy and selecting the final-state ``visible'' particles producing a minimum number of hits (allowed to be: $e^{\pm}$, $\mu^{\pm}$, $\gamma$, $\pi^{\pm}$, $\kappa^{\pm}$, $p$). Any downstream daughter particles are folded in these target particles.
\item \textit{Matching of reconstructed \twod hits to MCParticles:} Each reconstructed \twod hit is matched to the target MCParticle responsible for depositing the most energy within the region of space covered by the hit. The collection of \twod hits matched to each target MCParticle is known as its ``true hits''.
\item \textit{Matching of MCParticles to reconstructed PFParticles:} The reconstructed PFParticles are matched to target MCParticles by analyzing their shared \twod hits. A PFParticle and MCParticle will be matched if the MCParticle contributes the most hits to the PFParticle, and if the PFParticle contains the the largest collection of hits from the MCParticle. The matching procedure is iterative, such that once each set of matched particles has been identified, these PFParticles and MCParticles are removed from consideration when making the next set of matches. 
\end{itemize}

Using the output of this matching scheme, the following performance metrics can be calculated:

\begin{itemize}
\item \textit{Efficiency:} Fraction of MCParticles with a matched PFParticle,
\item \textit{Completeness:} The fraction of \twod hits in a MCParticle that are shared with its matched reconstructed PFParticle, and 
\item \textit{Purity:} The fraction of \twod hits in a PFParticle that are shared with its matched MCParticle.
\end{itemize}

\subsection{Reconstruction Performance in the DUNE \dword{fd}}
\label{sec:Pandora:DUNEFD}

The performance of the Pandora pattern recognition has been evaluated using a sample of accelerator neutrino interactions simulated using the reference DUNE neutrino energy spectrum and the \nominalmodsize \dword{detmodule} geometry. The following plots show that a good efficiency has already been achieved, and indicate particular regions and channels in which improvements can be made. 

\begin{figure}[!ht]
\centering
\includegraphics[width=0.5\textwidth]{Pandora-ALL_BUT_DIS_HitsEff_Tracks.png}\includegraphics[width=0.5\textwidth]{Pandora-ALL_BUT_DIS_HitsEff_Showers.png}
\includegraphics[width=0.5\textwidth]{Pandora-ALL_BUT_DIS_MomentumEff_Tracks.png}\includegraphics[width=0.5\textwidth]{Pandora-ALL_BUT_DIS_MomentumEff_Showers.png}

\caption[Reconstruction efficiency of Pandora pattern recognition for a range of final-state particles at the \dword{fd}]{The reconstruction efficiency of the Pandora pattern recognition obtained for a range of final-state particles produced in all types of accelerator neutrino interactions except deep inelastic ones at DUNE FD. The efficiency is plotted as a function of the total number of \twod hits associated with the final-state MCParticles (summed across all views) on the top row, and as a function of the true momentum of the particle on the bottom row. Plots are shown for track-like particles (left) and shower-like particles (right) of each type leading in the event (in addition to the sub-leading photon $\gamma_2$ added for reference to $\pi^0$ reconstruction).}
\label{pandora_particle_efficiency}
\end{figure}

Figure~\ref{pandora_particle_efficiency} shows the reconstruction efficiency as a function of the number of total true \twod hits and as a function of the true momentum for a range of final-state particles. The typical reconstruction efficiencies obtained for track-like MCParticles ($\mu^{\pm}$, $\pi^{\pm}$, $p$) rise from 65-85\% for simulated particles depositing 100\,hits to 85-100\% for particles with 1000\,hits. It should be emphasized that inefficiencies almost always result from accidental merging of multiple nearby true particles, rather than an inability to cluster hits from a true particle. The reconstruction efficiency for shower-like MCParticles ($e^{-}$,$\gamma$) is a bit lower than the equivalent for track-like particles at lower number of hits, but comparable with $>$100\,hits. The case of the sub-leading photon, $\gamma_2$, is particularly challenging given the correlation with the leading photon, $\gamma_1$, in the $\pi^0$ decays, which may result in accidentally merging both photons; therefore a loss of efficiency is expected for this particular channel, subject to improvements. 

Figure~\ref{pandora_completeness_purity} shows distributions of completenesses and purities for a range of final-state particles. In the case of final-state track-like particles, good completeness and purity are %both 
achieved, indicating that the track-based pattern recognition algorithms currently provide a high-quality reconstruction. It can be seen that final-state shower-like particles are typically reconstructed with high purity, but somewhat lower completeness, indicating that, although the shower reconstruction is fairly good already, there is room for addition of new algorithms specifically targeting an increase in shower completeness at DUNE.

For deep inelastic interactions, in which tens of final-state particles are produced, a breakdown such as in Figures~\ref{pandora_particle_efficiency} and~\ref{pandora_completeness_purity} \fixme{need verb here: is?} less representative and informative. Instead, Figure~\ref{pandora_dis} presents an assessment of the reconstruction of such events %is presented 
by comparing the number of reconstructed particles as a function of the number of true final-state particles in the event for \dword{nc} (left) and \dword{cc} (middle) deep inelastic interactions. These distributions are more populated in the diagonal, as they should be for perfect 1:1 reconstruction, indicating a good level of reconstruction of such events up to >5 final-state particles. In addition, the number of reconstructed particles matching the leading lepton in \dword{cc} deep inelastic interactions is also presented (right), which shows a consistently predominant single match for the leading lepton. 

Figure~\ref{pandora_vertex_resolution} shows distributions of the displacements $\Delta x$, $\Delta y$ and $\Delta z$ between the reconstructed and simulated neutrino interaction positions for all types of accelerator neutrino events. It can be seen that, for the vast majority of events, the reconstructed neutrino  interaction vertex lies within $2$\,cm of the \dword{mc} truth in $x$, $y$ and $z$. While the $\Delta x$ and $\Delta y$ distributions are both symmetrical and sharply peaked around the origin, a small forward bias can be seen in the $\Delta z$ distribution. The reason for this bias comes from the fact that the neutrino interaction will be boosted in the forward $z$ direction, so vertex candidates are more likely created at $\Delta z>0$ than $\Delta z<0$.  

\begin{figure}[!ht]
\centering
\includegraphics[width=0.5\textwidth]{Pandora-ALL_BUT_DIS_Completeness_Tracks_Log.png}\includegraphics[width=0.5\textwidth]{Pandora-ALL_BUT_DIS_Completeness_Showers_Log.png}
\includegraphics[width=0.5\textwidth]{Pandora-ALL_BUT_DIS_Purity_Tracks_Log.png}\includegraphics[width=0.5\textwidth]{Pandora-ALL_BUT_DIS_Purity_Showers_Log.png}
\caption[Completeness (top) and purities (bottom) for a range of final-state track-like  and shower-like particles]{Distributions of completenesses (top) and purities (bottom) for a range of final-state particles divided into track-like (left) and shower-like (right), produced in all types of accelerator neutrino interactions except deep inelastic ones at DUNE FD. }
\label{pandora_completeness_purity}
\end{figure}

\begin{figure}[!ht]
\centering
\includegraphics[width=0.315\textwidth]{Pandora-NCDIS.png}
\includegraphics[width=0.315\textwidth]{Pandora-CCDIS.png}
\includegraphics[width=0.315\textwidth]{Pandora-CCDISLeptonMatches.png}
\caption[Number of reconstructed particles per number of true final-state particles, \dword{cc} and \dword{nc}]{Distributions of number of reconstructed particles as a function of number of true final-state particles in deep inelastic events for neutral-current (left) and charged-current (middle) interactions. In addition, the number of reconstructed particles matching the leading lepton in charged-current deep inelastic interactions is also presented (right).}
\label{pandora_dis}
\end{figure}


\begin{figure}[!ht]
\centering
\includegraphics[width=0.35\textwidth]{Pandora-ALL_INTERACTIONS_VtxDeltaX.png}\includegraphics[width=0.35\textwidth]{Pandora-ALL_INTERACTIONS_VtxDeltaY.png}\includegraphics[width=0.35\textwidth]{Pandora-ALL_INTERACTIONS_VtxDeltaZ.png}
\caption[Displacements between reconstructed and simulated $\nu$ interaction vertices]{The displacements between the reconstructed and simulated neutrino interaction vertices. The distributions are plotted for $x$ (left), $y$ (centre) and $z$ (right) and include all types of accelerator neutrino interaction (also deep inelastic events).}
\label{pandora_vertex_resolution}
\end{figure}

\subsection{Reconstruction Performance in Single-Phase ProtoDUNE}
\label{sec:Pandora:ProtoDUNE}

Further examination of the performance of the Pandora pattern recognition is provided through studies of the test beam data taken by \dword{pdsp}.  Figure \ref{pandora_protodune_tbrecoeff} shows the reconstruction efficiency for triggered test beam particles as a function of the momentum recorded by the trigger.  The reconstruction efficiency metric folds in many effects, including reconstruction, removal of cosmic ray background and identification of the reconstructed particle as originating from the test beam.  An example of the Pandora reconstruction output for ProtoDUNE \dword{mc} simulations is shown in Figure~\ref{pandora_protodune_reco}.  A broad agreement between the \dword{mc} simulations and the data can be observed in Figure~\ref{pandora_protodune_tbrecoeff} across the range of momenta considered.  The exceptions to this are: (1) low-momentum events, where the low efficiency for data is due to particles interacting between the trigger and the LArTPC before reaching its active volume, an effect that is not simulated in \dword{mc}, and (2) high-momentum events, where the effect of test beam halo particles appears to be overestimated in the simulation.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.75\textwidth]{./Pandora/ProtoDUNE/BeamParticleEfficiencyVsMomentum.pdf}
\caption[Reconstruction efficiency for triggered test beam particle per particle momentum  and \dword{mc}.]{The efficiency of reconstruction for the triggered test beam particle as a function of the momentum of the test beam particle in (black) data and (red) Monte-Carlo.}
\label{pandora_protodune_tbrecoeff}
\end{figure}

\begin{figure}[!ht]
\centering
\subfloat[]{\label{fig:reco3d}\includegraphics[width=0.75\textwidth]{./Pandora/ProtoDUNE/Reconstruction.pdf}} \\ 
\subfloat[]{\label{fig:recou}\includegraphics[width=0.33\textwidth]{./Pandora/ProtoDUNE/ReconstructionU.pdf}}
\subfloat[]{\label{fig:recov}\includegraphics[width=0.33\textwidth]{./Pandora/ProtoDUNE/ReconstructionV.pdf}}
\subfloat[]{\label{fig:recow}\includegraphics[width=0.33\textwidth]{./Pandora/ProtoDUNE/ReconstructionW.pdf}}
\caption[Pandora reconstruction output for \SI{7}{GeV} \dword{mc} test beam event]{An example of the Pandora reconstruction output for a 7 GeV Monte Carlo test beam event.  Figure \protect\subref{fig:reco3d} shows the \threed reconstruction output for this event where the correctly reconstructed and tagged triggered test beam particle has been highlighted.  Figures \protect\subref{fig:recou}, \protect\subref{fig:recov} and \protect\subref{fig:recow} show the \twod hits for the reconstructed test beam particle where each coloured cluster of hits represents a different particle in the reconstructed particle hierarchy.}
\label{pandora_protodune_reco}
\end{figure}

The effect of cosmic ray backgrounds and the test beam particle halo on the reconstructed test beam particle efficiency is illustrated in Figure~\ref{fig:pandora_protodune_tbrecoeffbrkdwn}, where the efficiency is shown as a function of the momentum of the triggered particle (\ref{fig:pandora_protodune_tbrecoeffbrkdwn_p}) and the number of hits produced by the triggered particle (\ref{fig:pandora_protodune_tbrecoeffbrkdwn_nhits}).  These figures indicate that the primary loss mechanisms in the test beam particle reconstruction, accounting for $\approx 70\%$ of all inefficiencies, are due to irreducible cosmic ray and test beam halo backgrounds.

\begin{figure}[!ht]
\centering
\subfloat[]{\label{fig:pandora_protodune_tbrecoeffbrkdwn_p}\includegraphics[width=0.5\textwidth]{./Pandora/ProtoDUNE/BeamParticleEfficiencyBreakdownVsMomentum.pdf}}
\subfloat[]{\label{fig:pandora_protodune_tbrecoeffbrkdwn_nhits}\includegraphics[width=0.5\textwidth]{./Pandora/ProtoDUNE/BeamParticleEfficiencyBreakdownVsNHits.pdf}} \\
\subfloat[]{\label{fig:pandora_protodune_tbrecoeffbrkdwn_evt}\includegraphics[width=0.6\textwidth]{./Pandora/ProtoDUNE/EventComposition.pdf}}
\caption[Reconstruction efficiency for test beam particle in \dword{mc} per momentum and hits]{The efficiency of reconstruction for the triggered test beam particle in Monte-Carlo as a function of \protect\subref{fig:pandora_protodune_tbrecoeffbrkdwn_p} the triggered beam momenta and \protect\subref{fig:pandora_protodune_tbrecoeffbrkdwn_nhits} the number of hits made by the triggered particle.  The three curves show the reconstruction efficiency of the triggered test beam particle in isolation (blue), with beam particle halo overlaid (red) and with both beam particle halo and cosmic ray backgrounds overlaid (black).  Figure \protect\subref{fig:pandora_protodune_tbrecoeffbrkdwn_evt} shows the W plane view for a Monte-Carlo event where the triggered beam particle is shown in blue, the beam halo in red and the cosmic ray backgrounds in black.}
\label{fig:pandora_protodune_tbrecoeffbrkdwn}
\end{figure}

Alongside the test beam particle reconstruction metrics, the Pandora cosmic ray reconstruction has been studied on data. 
\fixme{studied on data?}
Figure \ref{fig:pandora_protodune_cr_n} shows the number of reconstructed cosmic rays per event.  Both data and \dword{mc} have a consistent average number of cosmic rays per event ($\approx 110$), however the \dword{mc} distribution seems to have a larger tail, which might be due to differences in the cosmic ray profile in the simulation with respect to the data. Figure~\ref{fig:pandora_protodune_cr_recovsmc} shows the number of reconstructed cosmic rays per event as a function of the number of ``target'' reconstructable (as explained in Section~\ref{sec:Pandora:assessment}) cosmic rays per event for \dword{mc} simulation, illustrating that the Pandora cosmic ray reconstruction is highly efficient.  The Pandora reconstruction is also able to tag the true time that a cosmic ray passes through the detector, $t_{0}$, should it cross a drift volume boundary, either \dword{cpa} or \dword{apa}.  This allows us to compare the $t_{0}$ distribution for tagged cosmic rays in data and \dword{mc}, shown in Figure \ref{fig:pandora_protodune_cr_t}.  There is excellent agreement between data and \dword{mc} in this instance.  For cosmic ray particles with a $t_{0} \approx 0$, the hits on either side of the \dword{cpa} or \dword{apa} boundary do not require shifting and therefore the algorithm tagging the $t_{0}$ is not activated; this explains the decrease in event fraction for the $t_{0} \approx 0$ bin.  Figure~\ref{fig:pandora_protodune_cr_tres} shows the resolution on the reconstructed $t_{0}$ for \dword{mc}, which indicates that the Pandora $t_{0}$ tagging is precise to the order of microseconds.  

\begin{figure}[!ht]
\centering
\subfloat[]{\label{fig:pandora_protodune_cr_n}\includegraphics[width=0.5\textwidth]{./Pandora/ProtoDUNE/NumberofReconstructedCosmicRays.pdf}}
\subfloat[]{\label{fig:pandora_protodune_cr_recovsmc}\includegraphics[width=0.357\textwidth]{./Pandora/ProtoDUNE/CRMatchesCosmicRayEvent.pdf}} \\
\caption[Reconstructed cosmic rays per event for data and \dword{mc}]{The number of reconstructed cosmic rays per event is shown in figure \protect\subref{fig:pandora_protodune_cr_n} for data and \dword{mc}.  Figure \protect\subref{fig:pandora_protodune_cr_recovsmc} shows the true number of reconstructable cosmic rays passing through the detector per event plotted against the number of reconstructed cosmic rays for \dword{mc}.}
\label{fig:pandora_protodune_cr_number}
\end{figure}

\begin{figure}[!ht]
\centering
\subfloat[]{\label{fig:pandora_protodune_cr_t}\includegraphics[width=0.5\textwidth]{./Pandora/ProtoDUNE/StitchedT0.pdf}}
\subfloat[]{\label{fig:pandora_protodune_cr_tres}\includegraphics[width=0.5\textwidth]{./Pandora/ProtoDUNE/Pandora_Resolution_T0_5GeV_Beam_Cosmic_SpaceChargeEffectOn.png}}
\caption[Distribution of reconstructed $t_{0}$ for cosmic rays]{Figure \protect\subref{fig:pandora_protodune_cr_t} shows the distribution of the reconstructed $t_{0}$ for cosmic rays crossing the \dword{cpa} in both data and \dword{mc}, while Figure \protect\subref{fig:pandora_protodune_cr_tres} shows the resolution on the reconstructed $t_{0}$ in \dword{mc}.}
\label{fig:pandora_protodune_cr_t0}
\end{figure}
%\subsection{Calorimetry Reconstruction in Single-Phase ProtoDUNE}
%\label{sec:caloprotodune}


\subsection{High-Level Reconstruction}
\label{sec:Pandora:High}

This section presents a series of studies to illustrate the results of current efforts on high-level reconstruction, analyzing different reconstructed quantities and calorimetric variables for tracks and showers. After the pattern recognition stage provided by Pandora, further fits to the reconstructed \threed particles can be made in order to characterize their properties. For the moment, the results presented here use only the output provided by Pandora, which includes a first pass of high-level reconstruction to build these objects. For tracks, Pandora sliding linear fits are used to calculate the trajectory of the particle, and for showers, a principal component analysis (PCA)\fixme{gloss?} is used to estimate directions and opening angles. 

\subsubsection{Tracks}
\label{sec:Pandora:High:Tracks}

Basic quantities that can be explored in the high-level reconstruction of track-like objects are length and direction (Figure~\ref{fig:pandora_dunefd_highlevel_reco}). Figure~\ref{fig:pandora_dunefd_highlevel_reco_length} shows the difference between reconstructed and true particle length (\threed distance between start and end positions) in simulated track-like particles of various types in the \dword{fd}. The same \dword{fd} simulated events are interrogated to produce Figure~\ref{fig:pandora_dunefd_highlevel_reco_angle}, which shows the angle between the reconstructed and the true \threed direction of the track.   

\begin{figure}[!ht]
\centering
\subfloat[]{\label{fig:pandora_dunefd_highlevel_reco_length}\includegraphics[width=0.5\textwidth]{./placeholder.png}}\subfloat[]{\label{fig:pandora_dunefd_highlevel_reco_angle}\includegraphics[width=0.5\textwidth]{./placeholder.png}}
\caption[Distribution of reconstructed-true length for different track-like particles]{Distribution of reconstructed - true length (\threed distance between start and end positions) for different track-like particles \protect\subref{fig:pandora_dunefd_highlevel_reco_length}, and angle between the reconstructed and the true \threed direction of the tracks \protect\subref{fig:pandora_dunefd_highlevel_reco_angle} in simulated \dword{fd} events.}
\label{fig:pandora_dunefd_highlevel_reco}
\end{figure}

A number of these variables can be also explored in real data taken by the \dword{pdsp} detector. In the case of the direction, the distribution of the opening angle between the expected direction and a fit to the start of the reconstructed test beam particle in \dword{pdsp} is shown for both data and \dword{mc} in Figure~\ref{fig:pandora_protodune_openingangle}.    Figure~\ref{fig:pandora_protodune_vertex} presents a measurement of the test beam particle interaction vertex by comparing the end point of the primary test beam particle track and the interaction vertex, obtained using the start point of the reconstructed daughter particles emerging from the interaction point, for \dword{pdsp} data and \dword{mc} events. 

\begin{figure}[!ht]
\centering
\includegraphics[width=0.75\textwidth]{./Pandora/ProtoDUNE/BeamParticleOpeningAngle.pdf}
\caption[distribution of opening angle between expected direction and fit to reconstructed test beam particle, data and \dword{mc}]{The distribution of the opening angle between the expected direction and a fit to the start of the reconstructed test beam particle for both data and \dword{mc}.  For data, the expected direction is given by the trigger, while for the \dword{mc} the truth information is used.  The discrepancy between the opening angles for data and \dword{mc}, particularly for low angles, is likely to be due to deviations in the expected direction for data as the particle travels from the trigger to the TPC active volume.}
\label{fig:pandora_protodune_openingangle}
\end{figure}



\begin{figure}[!ht]
\centering
\includegraphics[width=0.4\textwidth]{./placeholder.png}
\caption{Resolution on the test beam particle interaction vertex on \dword{protodune} data and \dword{mc} events, calculated comparing the end point of the primary test beam particle track and the interaction vertex, obtained using the start point of the reconstructed daughter particles emerging from the interaction point.}
\label{fig:pandora_protodune_vertex}
\end{figure}

 A data-driven measurement of the resolution of the reconstruction of track-like particles can be done by splitting the track in two parts, fitting both independently and comparing the results of both fits. This is presented in Figure~\ref{fig:pandora_protodune_trackres} for \dword{pdsp} cosmic tracks.
 
\begin{figure}[!ht]
\centering
\includegraphics[width=0.4\textwidth]{./placeholder.png}
\caption[Data-driven resolution measurement of the track reconstruction for \dword{pdsp} cosmic ray data]{Data-driven measurement of the resolution of the track reconstruction for \dword{pdsp} cosmic ray data, done by splitting the cosmic ray muon track in two parts, fitting both independently and comparing the results of both fits.}
\label{fig:pandora_protodune_trackres}
\end{figure}

%Distributions of dEdx for stopping muons and protons are shown in Figures\ref{fig:pandora_protodune_mu} and \ref{fig:pandora_protodune_proton} respectively... 
Cosmic ray muons in the \dword{pdsp} detector are also used to calibrate the detector nonuniformity and determine the absolute energy scale. Cathode crossing cosmic-ray muons with $t_{0}$ information are used to correct for the attenuation effect caused by impurities in the \lar. Stopping cosmic ray muons are used to determine the calorimetry constants that convert the calibrated \dword{adc} counts to the number of electrons so that the $dE/dx$ versus residual distributions match the expectation, as shown in Figures~\ref{fig:muon_dedx_resrange_run5387} and~\ref{fig:muon_dedx_resrange_sce} for \dword{pdsp} data and \dword{mc} simulation with space charge effects after calibration. The data $dE/dx$ distribution has better resolution because the purity in data is better than in the simulation. 

\begin{figure}[!ht]
\centering
\subfloat[]{\label{fig:muon_dedx_resrange_run5387}\includegraphics[width=0.33\textwidth]{graphics/muon_run5387_dedx.pdf}}
\subfloat[]{\label{fig:muon_dedx_resrange_sce}\includegraphics[width=0.33\textwidth]{graphics/muon_sce_mc.pdf}}
\subfloat[]{\label{fig:muon_comparison_dedx_sce_data}\includegraphics[width=0.33\textwidth]{graphics/muon_dEdx.pdf}}
\caption[Stopping muon $dE/dx$ distributions for the \dword{pdsp} cosmic data and \dword{mc}]{Stopping muon $dE/dx$ distributions for the \dword{pdsp} cosmic data and \dword{mc}. The red curves in \protect\subref{fig:muon_dedx_resrange_run5387} and \protect\subref{fig:muon_dedx_resrange_sce} are the expected most probable value of $dE/dx$ versus residual range.}
\label{fig:pandora_protodune_mu}
\end{figure}

The same attenuation correction and calorimetry constants are applied to the beam proton data and \dword{mc} and the resulting $dE/dx$ distributions are shown in Figure~\ref{fig:pandora_protodune_proton}. The data and \dword{mc} $dE/dx$ distributions agree well. Discrepancy with expectation is observed in the large residual range region, which corresponds to the beam entering point on the TPC front face where space charge effects are large. Good progress is being made on the space charge effects calibration, which will lead to more accurate $dE/dx$ measurements.
\begin{figure}[!ht]
\centering
\subfloat[]{\label{fig:proton_dedx_resrange_run5387}\includegraphics[width=0.33\textwidth]{graphics/proton_dedx_resrange_run5387.pdf}}
\subfloat[]{\label{fig:proton_dedx_resrange_sce}\includegraphics[width=0.33\textwidth]{graphics/proton_dedx_resrange_sce.pdf}}
\subfloat[]{\label{fig:proton_comparison_dedx_sce_data}\includegraphics[width=0.33\textwidth]{graphics/proton_comparison_dedx_sce_data.pdf}}
\caption[Proton $dE/dx$ distributions for the \dword{pdsp} 1 GeV beam data and \dword{mc}]{Proton $dE/dx$ distributions for the \dword{pdsp} 1 GeV beam data and \dword{mc}. The red curves in \protect\subref{fig:proton_dedx_resrange_run5387} and \protect\subref{fig:proton_dedx_resrange_sce} are the expected most probable value of $dE/dx$ vs residual range.}
\label{fig:pandora_protodune_proton}
\end{figure}


\subsubsection{Showers}
\label{sec:Pandora:High:Showers}

For shower-like particles, different quantities are presented for DUNE \dword{fd} simulated events in Figure~\ref{fig:pandora_dunefd_showers}. The first row of plots shows $dE/dx$ calculated at the start of the shower (within the first \SI{2}{cm} from its start point) for electrons (Figure~\ref{fig:pandora_dunefd_showers_dEdx_e}) and photons (Figure~\ref{fig:pandora_dunefd_showers_dEdx_ph}) %This will need further explanation of features observed for sure - to do when I have the final plot (i.e. might be only non DIS events, collection plane, etc.  
The difference between the true and reconstructed energy is presented in Figure~\ref{fig:pandora_dunefd_showers_E_e} for electrons and in Figure~\ref{fig:pandora_dunefd_showers_E_ph} for photons. Finally the opening angle between the true and reconstructed direction of the shower is shown in Figure~\ref{fig:pandora_dunefd_showers_angle_e} for electrons and Figure~\ref{fig:pandora_dunefd_showers_angle_ph}  for photons. 

\begin{figure}[!ht]
\centering
\subfloat[]{\label{fig:pandora_dunefd_showers_dEdx_e}\includegraphics[width=0.35\textwidth]{./placeholder.png}}\subfloat[]{\label{fig:pandora_dunefd_showers_dEdx_ph}\includegraphics[width=0.35\textwidth]{./placeholder.png}}

\subfloat[]{\label{fig:pandora_dunefd_showers_E_e}\includegraphics[width=0.35\textwidth]{./placeholder.png}}\subfloat[]{\label{fig:pandora_dunefd_showers_E_ph}\includegraphics[width=0.35\textwidth]{./placeholder.png}}

\subfloat[]{\label{fig:pandora_dunefd_showers_angle_e}\includegraphics[width=0.35\textwidth]{./placeholder.png}}\subfloat[]{\label{fig:pandora_dunefd_showers_angle_ph}\includegraphics[width=0.35\textwidth]{./placeholder.png}}
\caption[Distributions of $dE/dx$, energy and angle between true and reconstructed directions for $e^-$s and $\gamma$s]{Distributions of $dE/dx$ (top), energy (middle) and angle (bottom) between true and reconstructed directions for electrons (left) and photons (right).}
\label{fig:pandora_dunefd_showers}
\end{figure}


Using \dword{pdsp} electron test beam data, some of the previous quantities can be computed for real data as well. This is shown in Figure~\ref{fig:pandora_protodune_showers_dEdx_e} for $dE/dx$, and in Figure~\ref{fig:pandora_protodune_showers_dEdx_e} for energy in each of the different momentum ranges.   

\begin{figure}[!ht]
\centering
\subfloat[]{\label{fig:pandora_protodune_showers_dEdx_e}\includegraphics[width=0.4\textwidth]{./placeholder.png}}\subfloat[]{\label{fig:pandora_protodune_showers_E_e}\includegraphics[width=0.4\textwidth]{./placeholder.png}}
\caption[Distributions of $dE/dx$, energy and angle between true and reconstructed directions for $e^-$s and $\gamma$s????]{
Distributions of $dE/dx$ (top), energy (middle) and angle (bottom) between true and reconstructed directions for electrons (left) and photons (right).}
\label{fig:pandora_protodune_showers}
\end{figure}
\fixme{same caption for both figures}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Tools and Assumptions use for Evaluation of \dword{nd} Capabilities}
\label{sec:tools-nd-eval}

\fixme{This section is intended to provide some common descriptions with application to later chapters.  For now, the relevant chapters contain sufficiently complete standalone descriptions.  We may leave it that way.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Appendix}
\label{sec:tools-appendix}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Additional Flux Modeling Details}
\label{sec:tools-app-flx}

\subsubsection{Off-axis Neutrino Flux and Uncertainties}
\label{sec:tools-app-flx-offaxis}

The neutrino flux has a broad angular distribution and extends outward at the \dword{nd} hall. At an ``off-axis'' angle relative to the initial beam direction, the subsequent neutrino energy spectrum is narrower and peaked at a lower energy than the on-axis spectrum.
The relationship between the parent pion energy and neutrino energy is shown in Figure~\ref{fig:OAAFluxFigs}.  At $575\,\textrm{m}$, the location of the \dword{nd} hall, a lateral shift of $1\,\textrm{m}$ corresponds to approximately a $0.1^\circ$ change in off-axis angle.

\begin{dunefigure}[$\nu$ energy as  function of parent pion energy for different angles away from the pion momentum vector]{fig:OAAFluxFigs}
{(left) The neutrino energy as a function of parent pion energy for different angles away from the pion momentum vector. Figure from Ref.~\cite{Duffy:2016owt}. (right) The DUNE near detector flux predictions over a range of off-axis positions for a near detector at $575\,\textrm{m}$ downstream of the target station. }
%is subfig package included? I don't think subfloat is working (ZP)
%  \subfloat[C][Off-axis pion decay kinematics]{
    \includegraphics[width=0.4\textwidth]{OATrick}
%    \label{fig:OATrick}
%  }
%  \subfloat[][DUNE near detector flux predictions]{
    \raisebox{0.5em}{\includegraphics[width=0.45\textwidth]{OffAxisFluxes_1D}}
%    \label{fig:OffAxisFluxes_1D}
%  }
\end{dunefigure}

The intrinsic neutrino flavor content of the beam varies with off-axis angle. Figure~\ref{fig:OffAxisFluxes_1D_AllSpec} shows the neutrino-mode and anti-neutrino-mode predictions for the four neutrino flavors at the on-axis position, and a moderately off-axis position. At the $30\,\textrm{m}$ position, a second, smaller energy peak at approximately \SI{4}{GeV} is due to the charged kaon neutrino parents. 
%As both the pion and kaon-parent peaks are significantly narrower in observed neutrino energy at greater off-axis angle, this which may allow for off-axis kaon-parent analyses.

\fixme{Fig~\ref{fig:OffAxisFluxes_1D_AllSpec} is partly redundant with earlier figure, nice if it can be merged? }

\begin{dunefigure}[The predicted muon neutrino energy spectra at two near detector positions, on axis and $30\,\textrm{m}$ off axis.]{fig:OffAxisFluxes_1D_AllSpec}
{The predicted muon neutrino energy spectra at two \dword{nd} positions, on axis and $30\,\textrm{m}$ off axis. (a) The predicted neutrino flavor-content of the neutrino-mode (FHC) and anti-neutrino-mode (RHC) beam. (b) The neutrino-mode, muon-flavor predicted flux, separated by the particle that decayed to produce the neutrino. The off-axis spectrum displays a double peak structure due to charged kaon parent decay kinematics. The on-axis kaon-peak occurs at higher neutrino energy and will have a significantly broader energy spread. Top: Beam neutrino flavor content, middle: Beam neutrino flavor content; bottom: Beam neutrino decay-parent species}
   % \includegraphics[width=0.8\textwidth]{OffAxisFluxes_1D_AllSpec}
    \includegraphics[width=0.8\textwidth]{OffAxisFluxes_1D_AllSpec}
  \includegraphics[width=0.8\textwidth]{OffAxisFluxes_1D_Parents}    
    \end{dunefigure}

\fixme{one label per dunefigure; I didn't use label fig:OffAxisFluxes\_1D\_Parents or fig:OAAFluxFigsDetail - may need to fix refs}

The same sources of systematic uncertainty that affect the on-axis spectra also modify the off-axis spectra.  Generally, the size of the off-axis uncertainties is comparable to the on-axis uncertainties and the uncertainties are highly correlated across off-axis and on-axis positions. Studies of extreme changes to horn positions and current indicate that the larger-than-expected shifts to the horn focusing elements (as has been historically seen in NuMI) would induce significant changes off-axis, and so off-axis flux measurements are useful to diagnose beamline physics.

\fixme{DISCUSS FIGURE: Plot of extreme off-axis angle 1D uncertainty profiles? Can be relative off-axis to on-axis or just off-axis only. }

%\begin{dunefigure}[On-axis and off-axis flux %uncertainties]{fig:onvsoff_flux_uncertainty}
%{}
%
%    \includegraphics[width=0.8\textwidth]{on_v}
%\end{dunefigure}
%\ifdefined\isfinal\documentclass[final]{dune}\else\documentclass{dune}\fi
%\pdfoutput=1            % must be in first 5 lines so arXiv finds it
%\graphicspath{ {graphics/} {executive-summary/graphics/}{generated/} }
%% * <aheavey@fnal.gov> 2018-04-06T17:26:26.700Z:
%%
% ^.
%\input{common/preamble}

% define a local dword for later incorporation
\newcommand{\ldword}[1]{{\bf{[#1]}}\footnote{define word#1}}
\newcommand{\ldshort}[1]{{\bf{[#1]}}\footnote{define abbr #1}}
\newcommand{\ignore}[1]{}
\newcommand{\lcite}[1]{
{#1}}
\renewcommand\thedoctitle{\voltitleexec} % defined in common/defs.tex
%newcommand\thevolumenumber{1}
%\def\titleextra{\includegraphics[width=0.55\textwidth]{energy_nu_no}} -- change image file

%\begin{document}

%%%%%%%%%%%%%%%%%%%%  REMOVE ABOVE %%%%%%%%%%%%%%%%%
%\newduneword{rucio}{Rucio}{Data management system originally developed by ATLAS but now open-source and shared across HEP}
%\newduneabbrev{doma}{DOMA}{Data Organization, Management, and Access}{Data Organization, Management, and Access efforts through the HEP Software Foundation}
%\newduneabbrev{hsf}{HSC}{High Energy Physics Software Foundation}{High Energy Physics Software Foundation}
%\newduneabbrev{wlcg}{WLCG}{Worldwide LHC Computing Grid}{Worldwide LHC Computing Grid}
%\newduneabbrev{osg}{OSG}{Open Science Grid}{Open Science Grid}
%\newduneabbrev{sci}{SCI}{Scientific Computing Infrastructure}{Proposed extension of the infrastructure component of \dword{wlcg} to other experiments}
%\newduneabbrev{csc}{CSC}{Computing and Software Consortium}{DUNE Computing and Software Consortium}


\chapter{Computing in DUNE}
\label{ch:exec-comp}
%
%\fixme{Heidi, this outline may be overkill for the exec summary; it may be a good structure for the computing CDR volume, then pared down for inclusion here. My 2 cents! -Anne}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{To remove: just examples}
%\label{sec:exec-comp-1}
%
%Sample figure to copy and edit, Figure~\ref{fig:map}:
%
%\begin{dunefigure}[DUNE collaboration global map]{fig:mhexec}{The international DUNE
%collaboration. Countries with DUNE membership are shown in orange.}
%\includegraphics[width=0.9\textwidth]{global-retouched.jpg}  
%\label{fig:map}
%\end{dunefigure}
%
%Sample table to copy and edit, Table~\ref{tab:execosctable}:
%
%\begin{dunetable}[Required exposures to reach oscillation physics
%  milestones]{lcc}{tab:execosctable}{The exposure in mass (kt) $\times$ proton beam power
%    (MW) $\times$ time (years) and calendar years assuming the staging plan described in this chapter needed to reach certain oscillation physics
%    milestones. The numbers are for normal hierarchy using the NuFit 2016 best fit values of the known oscillation parameters.  }
%Physics milestone & Exposure  & Exposure \\ \rowtitlestyle
%  & (\ktMWyr{}) & (years)  \\ \toprowrule 
%  $1^\circ$ $\theta_{23}$ resolution ($\theta_{23} = 42^\circ$) & 29  &  1\\ \colhline
%  CPV at $3\sigma$ ($\delta_{\rm CP} = -\pi/2$)  & 77 &  3\\ \colhline
%  \dword{mh} at  $5\sigma$ (worst point) & 209 & 6 \\ \colhline
%  $10^\circ$ $\delta_{\rm CP}$ resolution ($\delta_{\rm CP} = 0$) & 252 & %5 
%  6.5 \\ \colhline
%  ($\sin^2 2 \theta_{13} = 0.084 \pm 0.003$) &  &  \\  
%\end{dunetable}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Executive Summary}		
\label{ch:exec-comp-es}







The DUNE long baseline neutrino oscillation collaboration consists of 178 institutions from 32 countries, including 15 European nations and CERN. The experiment is in preparation now with commissioning of the first module expected over the period 2024-2026 and a long data taking run with 4 modules expected from 2026-2036 and beyond.  An active prototyping program is already in place with a short test beam run with a 700T, 15,360 channel prototype of single-phase readout at the neutrino platform \dword{cern} in late 2018 and tests of a similar sized dual-phase detector scheduled for mid-2019.   The DUNE experiment has already  benefited greatly from these initial tests.  The collaboration has recently formed a formal Computing Consortium, with significant participation by European Institutions and interest from groups in Asia to work on common software and computing development and to formalize resource contributions.

The consortium resource model benefits from existing Grid and \dword{wlcg} infrastructure developed for the LHC.  DUNE, through  the ProtoDUNE-SP effort, is already using global resources for simulation and the analysis of ProtoDUNE-SP data.  Multiple European sites are part of this resource pool and are making significant contributions to the ProtoDUNE single and dual phase programs.  We expect this global computing consortium to grow and evolve as we move towards data from the full DUNE detectors in the middle of the next decade.

The DUNE science program is expected to produce raw data volumes similar in scale to the data volumes that current LHC Run-2 experiments have already recorded.  Baseline predictions for the DUNE data, dependent on actual detector performance and noise levels, are 30-60 PB of raw data per year.  These data, with simulations and derived analysis samples, need to be available to all collaborating institutions.  We anticipate that institutions worldwide will play an important role both as contributors and end-users of storage and CPU resources for DUNE.

To enable these resource contributions in cooperation with the LHC and other communities, we plan to utilize common computing layers for infrastructure access and use common tools to ease integration of facilities with both the DUNE and LHC computing ecosystems.  We will use common data storage methodologies to establish large highly available data lakes worldwide  and to collaborate with the broader HEP community in developing other common tools.


HEP has considerable infrastructure in place for international computing collaboration thanks to the LHC program.  Other large experiments - LSST, SKA, DUNE and HyperK will be coming on board over the next decade.   This cooperation is being formalized through the HEP Software Foundation(\dshort{hsf})\lcite{Alves:2017she}, an organization of interested parties working to gather and disseminate common solutions based on the extensive knowledge we have gained over the past two decades. 
DUNE's strategy is to work with and contribute to this global community to maximize the use of common tools for data movement and storage, job control and monitoring, accounting and authentication.   All large-scale experiments will encounter similar issues and worldwide cooperation on common tools is the most cost-effective way to proceed. For example, in collaboration with the Fermilab, CERN and the UK groups, we are investigating the use of {\it \dword{rucio}}  as our primary data manager.

The 2018 test beam run of ProtoDUNE Single-Phase (SP) was a valuable live test of this model.  The ProtoDUNE Single Phase detector at CERN produced raw data at rates of up to ~2GB/s of data.  These data were stored on tape at CERN and Fermilab and replicated at sites in the UK and Czech Republic.  In total 1.8 PB of raw data were produced during the test beam run. 
This prototype run has been extremely beneficial in exercising the existing computing infrastructure and in building a team of interested institutions.


In addition to traditional HEP computational strategies, DUNE's data consists of simple but very large 2D data objects which share many characteristics with astrophysical images.  This presents opportunities to use current advances in machine learning and pattern recognition as a frontier user of High Performance Computing facilities capable of massively parallel processing.  We share this problem, and propose to share solutions, with the other Liquid Argon experiments - ArgoNeut, MicroBooNE, SBND and ICARUS.  We have already benefited greatly from prior work and plan to contribute cooperatively.

In summary, DUNE's computing strategy is to be {\bf global}, working with partners worldwide, and {\bf collaborative}, as almost all of the computational challenges we face are faced by similar experiments. 
 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Overview}		
\label{ch:exec-comp-ovr}
The main goal of the computing effort is to facilitate the acquisition, processing and analysis of data and simulations from the DUNE experiment across the many physics drivers for the experiment in a cost effective and secure way. Computing and Software provides the bridge between the real-time online systems of the DUNE/LBNF hardware and the physics groups who develop high-level algorithms and perform data analysis. S+C works with collaborating institutions to identify CPU and storage resources and to support basic software infrastructure such as software frameworks, data catalogs, database infrastructure and code distributions systems. 

The Consortium is currently working with national agencies and major laboratories to negotiate CPU and storage provision for the near term ProtoDUNE runs and development of the full DUNE computing model and is starting the process of evaluating major software infrastructure systems such as workload management, emphasizing collaboration and reuse of existing systems. 

Our initial assessment of needs indicates that data rates and CPU needs for DUNE are significantly less than those for the large LHC experiments but that the extremely large size of individual DUNE events presents unique technical challenges that will require substantial effort to address.  In addition, the collaboration is truly international and will require a distributed computing model to both fully exploit our global computing resources and to make those resources easily available to all collaborators. 


\section{Data types and volumes}
Maximum raw data volumes from the detectors are reasonably easy to estimate, as a product of trigger rate, number of channels, \# of time slices/channel,  size for each ADC readout and compression. However, those data sizes would be well beyond the ability of any system to handle. Thus decisions must be made about what to keep and what not to keep. Those decisions are the purview of the collaboration scientists and the data acquisition design with feedback from computing on what is possible.  The ProtoDUNE experience has provided invaluable information to feed back to the experiment design. 




\subsection{Far detector}

The computing model needs to be able to handle a wide range of data inputs from the far detectors, as documented in more detail in docdb-9240\lcite{bib:docdb9240}.

\begin{itemize}
\item Supernova triggers which would have an uncompressed size of 138 TB for a 30 second readout of all channels in a 4-module single-phase detector at a likely rate of 1/month.  
\item Beam neutrino interactions within a single detector module with an uncompressed size of $\approx$ 6.2 GB at a rate of up to 1 Hz
\item Atmospheric neutrino interactions, nucleon decay and other lower energy processes confined to a subset of a detector module with a low threshold largely driven by radiological backgrounds.
\item Cosmic ray and rock muons at a rate of around 4,500/day/module.
\item Other calibration systems 
\end{itemize}

The estimates in docdb-9240, with conservative estimates for increased needs for low level data taking during commissioning, have led to a negotiated upper limit of 30 PB/year data volume as a standard for both the trigger and data acquisition and computing groups to work towards. 


\begin{dunetable}[Useful quantities for computing estimates]{lrrr}{tab:exec-comp-bigpicture}{Useful quantities for computing estimates}%\rowtitlestyle
Quantity&Value&Units&Explanation\\ 
\hline
{\bf Far Detector Beam:}\\
Single APA readout &41.5& MB& Uncompressed 5.4 ms\\
APAs per module& 150& &\\
Full module readout &6.22&  GB& Uncompressed 5.4 ms\\
Beam rep. rate&\beamreprate& Hz&Untriggered\\
CPU time/event&600-1,200&sec&from MC/ProtoDUNE\\
Memory footprint&2-4&GB&ProtoDUNE experience\\
\hline
{\bf Supernova:}\\
Single channel readout &90.0&MB& Uncompressed 30 s\\
Four module readout&138.2& TB& Uncompressed 30 s\\
Trigger rate&1 & per month&(assumption)\\
%Yearly rates nd
%Reduction with roi.  
%CPU time/ event for reconstruction
%Reduction for analysis
%Users 
\end{dunetable}

\subsection{Near Detector}
In addition, a near detector of reasonable size will have multiple neutrino interactions/beam spill leading to a need to read out at the full beam rate of 0.8-1.2 Hz.
The near detector will have fewer channels and better signal/noise discrimination but much higher readout rates.  While the details of the detector design are still unknown, we assume data volumes of similar size to the far detector (30PB/year) in our planning.

\subsection{Simulation}
The bulk of data collected is likely to be backgrounds, with real high energy events in the far detector numbering in the thousands/year, not millions. Thus the size of simulation samples is likely to be less than that of the raw data.  Lower energy events are either very rare or can be simulated in sub-volumes of the whole detector.  As a result, while simulation will be an important part of the experiment, it is not expected to dominate data volumes as it does in many experiments.  

However, simulation inputs such as flux files, overlay samples and shower libraries pose a special problem as they must be distributed to simulation jobs carefully.  Proper simulation requires that these inputs be delivered in an unbiased fashion. This can be technically difficult in a widely distributed environment and will require thoughtful design. 

\subsection{Analysis}

Analysis formats have not yet been fully defined.  We anticipate that most analysis samples will be orders of magnitude smaller than the raw data.  However, as they are idiosyncratic to particular analyses and in fact particular users,  producing and cataloging them will be sociologically difficult. 
Likely there will be a mix of official samples -  produced by physics groups and distributed through a common catalog and file transfer mechanisms - and user samples on local disk. 


\section{ProtoDUNE-SP as an example}		
\label{ch:exec-comp-proto-SP}
The first ProtoDUNE single phase run at CERN in late 2018 has already led to a small-scale test of a global computing model.  In the following we will describe the ProtoDUNE data design and the lessons learned from our experience. Much of this carries over into planning for full far-detector operations. 

\subsection{Introduction}

The ProtoDUNE Single Phase detector ran at CERN in the np04 beamline from September to November of 2018. Since then, studies with cosmic rays have continued. Prior to that run there were several data challenges at high rate to validate the data transfer mechanisms. 

The first phase of operations was commissioning of the detector readout systems while the argon reached full purity.  Data were taken with cosmic rays and beam during the commissioning period.  Physics data were then taken with beam at full  Argon purity through October and half of November.  Since beam was turned off, cosmic ray data continues to be taken with varying detector conditions, such as modified high voltage and purity and new readout schemes. 



\subsection{Data volumes}
The single-phase ProtoDUNE detector consists of a \dshort{tpc} with  6 Anode Plane Assemblies (\dshort{apa}), photon detectors (\dshort{pd}s) and a Cosmic Ray Tagger (\dshort{crt}). In addition, the np04 beamline is instrumented with hodoscopes and Cerenkov counters to generate beam triggers and random readouts were done at lower rates to collect unbiased cosmic ray information. The data volume from the test beam run was dominated by readout of the \dshort{TPC}.  Each \dshort{apa} has 2,560 channels and reads out 12 bit ADC values at 2 MHz.  During beam running, the detector was triggered on the incoming beam. The nominal readout window during beam running was  3 ms to match the drift time at the full voltage of 180 kV that was maintained for most of the run.  The total size of the \dshort{tpc} data without compression was thus 138 MB/event.  Compression was implemented before the October beam physics run, lowering the total size per event from around 180 MB to 75 MB.  

\begin{dunetable}[Data volumes]{lrr}{tab:exec-comp-pd-volumes}{Data volumes  recorded by ProtoDUNE-SP}
Type  & Events & Size\\ %\rowtitlestyle
Raw Beam&8.08 M& 520 TB \\
Raw Cosmics&3.46 M& 271 TB\\
Commissioning&3.86 M& 388 TB\\
Pre-commissioning&13.89 M&641 TB\\
\end{dunetable}

Events were written out in raw files of size 8 GB with each containing of order 100 events. The beam was live for two 4.5 s spills every 32 s beam cycle and data were taken at peak rates of up to 50 Hz (typically 25 Hz) leading to compressed DC rates out of the detector of 400-800MB/sec.  Each beam cycle could therefore produce 1-4  8 GB output files.  In earlier running with uncompressed data, and during an April data challenge, transfer rates of up to 2GB/s were demonstrated over substantial periods. 

Beam stopped in November 12 but cosmic ray studies of the detector continue, some with an increase time window of 7.5 ms to collect more complete tracks/readout.  This raises the compressed event size to around 170 MB.


\subsection{ProtoDUNE-SP data streams}
The ProtoDUNE-SP data consist of multiple sources in addition to the TPC data. One of the major challenges for the offline computing systems is merging of these multiple streams into a coherent whole for analysis.  Table \ref{tab:exec-comp-pd-sources} lists the data sources used and their granularity. 

\begin{dunetable}[Data sources]{lrr}{tab:exec-comp-pd-sources}{Data sources  }
Type & indexed by & destination\\
TPC  & run/event & event data\\
Photon Detector data & run/event & event data\\
Cosmic Ray Tagger & run/event & event data\\
Beamline devices & time-stamp & beam database\\
Detector conditions & time-stamp & slow controls database\\
DAQ configuration & run & files/elisa logbook\\
Run quality & run & human generated spreadsheets\\
Data quality & run/event/time & Data Quality web application\\
File metadata & file & \dword{sam} file database\\
\end{dunetable}

Information about the detector conditions, \dword{daq} configuration and run quality is spread across a number of sources and must be collected and then boiled down into the quantities relevant for offline data analysis.  For example, the Slow Controls system logs detector conditions continually.  Offline analysis needs to know about these data with coarser granularity and then have algorithms capable of using that information. A full conditions database transfer mechanism is being developed but was not available during the run.  As a result, with the exception of beamline information, coarse information is currently added to the \dword{sam} file catalog run by run to allow files with given operating conditions to be easily identified and retrieved. Beam data is stored in the \dword{ifbeam}
database and connected to event data via time stamps.

\section{Reconstruction of ProtoDUNE-SP data}
Thanks to substantial previous effort by the 35T prototype, \dword{microboone} and the liquid argon community, high quality algorithms were already in place to reconstruct the TPC  data.  As a result, a first pass reconstruction of the ProtoDUNE-SP data with beam triggers was completed by early December, less than a month after the end of data taking.

%\subsection{Reconstruction}
%
%The TPC data read from the ProtoDUNE detector includes a waveform for
%each of 15,360 channels. Each waveform is the output of a 12-bit ADC sampling
%at 2~MHz the output voltage of a charge-sensitive amplifier connected to one of
%the TPC Wires. 
%%The most interesting contribution to these signals is the
%charge on the wire induced by the motion of electrons in the TPC.
%There are also deliberate voltage offsets to put the signal in the amplifier
%and ADC ranges and there is noise from the electronics (jitter and pickup)
%and wire motion. Finally, each ADC channel has response nonlinearity and
%distortions.
\ignore{
The electrons produced in the \dword{tpc} volume drift % away from a cathode plane
toward three planes of anode wires with voltage bias such that the electrons
pass through the first two planes (called induction planes) and are collected
on the last (collection plane).
The signal induced on the collection plane wires is unipolar 
and the pedestal-subtracted area of the signal is roughly proportional to the
charge collected on that wire.
The signals on the induction wires are bipolar: first a positive signal is produced as
electrons approach the plane and then negative as they move away.
}
%The time (i.e. waveform sample) at which the signal appears provides a measure of the drift
%time which is used to deduce the drift distance.
\subsection{Data preparation}

Before pattern recognition, data from the ProtoDUNE detector is
unpacked and copied to a standard format.
The same format is produced in detector simulation.
This reformatted raw data includes the waveform for each channel, an
integer in the range [0, 4095] for each of 6000-15,000 time slices. 

The first step in reconstruction is data preparation with the goal of
converting each ADC waveform into a calibrated charge waveform with
signals proportional to charge. At the end of data preparation, regions of interest (ROIs), i.e. blocks of contiguous samples where
useful signals appear, are identified and the data outside these regions are discarded.

%Perhaps most important, the bipolar signals in induction wires are made unipolar.
%Also the electronics shaping is replaced with the Gaussian shaping expected in the
%next stage of processing.
%Relative channel-to-channel and absolute calibration is applied to account for the
%responses of the amplifiers and ADCs.
%And attempts are made to remove noise and mitigate ADC distortions.

The sequence is described more fully in docdb-12349\lcite{ref:docdb12349} and in the Methods section of the Physics Volume but is summarized here:

\begin{enumerate}
\item Each waveform is unpacked into integers.
\item Pedestals are determined per event/per channel from the most common ADC value. 
\item Pedestals and calibrations are applied. %\label{local:ped}
\item Bad channels, sticky bits and other know hardware problems are corrected or removed.
\item Signal undershoot that creates a long negative tail is removed. 
\item The waveforms  are deconvoluted.  In the first processing this was done with simple 1-D convolution for a single wire but in future the 2-D convolution described by the MicroBooNE collaboration in \lcite{Adams:2018dra} will be used to eliminate cross-talk with adjacent wires.  The deconvolution Fourier transforms the waveform, applies a response function, applies a low pass filter to remove high frequency noise and then transforms back.



\item Finally regions of interest are defined where the signal exceeds a given threshold and time slices well outside the ROI are dropped, leading to significant reduction in the size of the remaining data. These data are then fed into the reconstruction algorithms for further pattern recognition. %\label{local:roi}
\end{enumerate}




%\subsection{Signal processing}
Figures~\ref{fig:ch-exec-comp-chtraw}-\ref{fig:ch-exec-comp-chtroi} illustrate the transformation of TPC data  during data
preparation.

\begin{figure}[t]
\includegraphics[width=\textwidth,angle=0]{comp-evd_twq-proj_5449_20926_raw.png}
\caption{
Example of pedestal-subtracted data for one of the ProtoDUNE  wire planes.  The top pane shows the ADC values in a V (induction) plane with the x-axis being channel number and the y-axis, time slice. The bottom pane shows the bipolar pulses induced on one channel. 
}
\label{fig:ch-exec-comp-chtraw}
\end{figure}




%\begin{figure}[t]
%  \includegraphics[width=\textwidth]{ccomp-evd.twq-proj.5449.20926.recon..png}
%\caption{
%Same as Fig.~\ref{fig:ch-exec-comp-chtraw} except after hit correction, tail removal and deconvolution.
%}
%\label{fig:ch-exec-comp-chtdco}
%\end{figure}

\begin{figure}[t]
  \includegraphics[width=\textwidth]{comp-evd_twq-proj_5449_20926_decon.png}
\caption{
Same as Fig.~\ref{fig:ch-exec-comp-chtraw} except after calibration, cleanup, deconvolution and ROI finding. 
}
\label{fig:ch-exec-comp-chtroi}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\todo{Statement about timing and memory for this phase}

\subsection{Computational characteristics of data preparation and deconvolution }
Decoding for ProtoDUNE-SP is currently done with all 6 APAs in memory. As each 3 ms APA readout consists of over 15M 12 bit values, decompression and conversion to floating point results in substantial memory expansion.  Decoding and deconvolution of 6 APAs with 3 ms readout fits within a normal 2 GB memory/core footprint but the 7.5 ms readout window used in some cosmic ray studies requires a correspondingly larger memory footprint. As electrical signals are correlated between channels within an APA wire plane, but not between planes, better memory performance can be achieved by processing each wire plane (3/APA) independently. These changes are being implemented.


However,  while subdividing the detector into wire planes solves the memory problems for short readouts it is  not a viable solution for the long readouts expected for supernova events. We are still exploring the best strategy for dealing with these much larger ($\times 10,000$)time windows. The DAQ group is already testing 1 second ($300 \times$ longer time window) readouts of small numbers of channels.  These are being used as tests of optimal models for data segmentation.  Section \ref{ch:exec-comp-mod} describes the start of a bottoms-up collaboration with the \dshort{daq} consortium on an optimal data model for the full DUNE detectors. 

\subsection{Further reconstruction}
The downstream pattern recognition steps starting with \dword{roi} are described further in the Tools and Methods chapter of the Physics Volume.  
Full reconstruction of ProtoDUNE-SP interactions, with beam particles and of order 20-40 cosmic rays per readout window took 600-1200 sec/event.
Table  \ref{tab:comp-raw-data-size} shows the input datasize for a typical beam event, dominated by around 71 MB of TPC waveform information. Table  \ref{tab:comp-reco-data-size} shows the size of different reconstructed objects, still dominated by around 10 MB of reduced TPC hit information,  while \ref{tab:comp-reco-data-time} shows the reconstruction time breakdown.  This event had a 3 ms readout window.  The input size and reconstruction time scale reasonably linearly with the readout window.  

\subsection{Reconstruction characteristics}

The data preparation phase can be segmented by detector component, for example into wire planes within a APA  The operations performed in signal processing require few decisions to be made but do include operations such as fast-Fourier transforms and deconvolution.  These operations are well suited to GPU's. 

Once ROI's have been identified, several 3-D reconstruction packages are used. For the first reconstruction pass in November, the  \dword{pandora}\cite{Acciarri:2017hat}, \dword{wirecell}\cite{wirecell} and \dword{pma}\cite{ref:PMA}  frameworks were used with results described in the Physics volume.   Table \ref{tab:comp-reco-data-time} indicates that they are comparable in terms of CPU time used.   Deep Learning techniques based on image pattern recognition algorithms are also being developed. Many of these algorithms can be adapted to run on HPC's, but probably different architectures that would be optimal for the data preparation phase. 

All of these algorithms are currently being run on conventional unix CPU's using \dword{osg}/\dword{wlcg} grid computing  infrastructure. 



\begin{dunetable}[Compressed size for Raw data - 7 GeV beam events with a 3 ms time window]{rrl}{tab:comp-raw-data-size}{Compressed size for Raw data - 7 GeV beam events with a 3 ms time window}
Size in Kbytes	&	Fraction	&	Data Product Name	\\
\hline
44,155.47	&	0.576	&	artdaq::Fragments\_daq\_ContainerTPC\_DAQ.	\\
27,952.64	&	0.364	&	artdaq::Fragments\_daq\_ContainerFELIX\_DAQ.	\\
4,586.82	&	0.06	&	artdaq::Fragments\_daq\_ContainerPHOTON\_DAQ.	\\
5.72	&	0	&	artdaq::Fragments\_daq\_ContainerCTB\_DAQ.	\\
0.17	&	0	&	artdaq::Fragments\_daq\_TIMING\_DAQ.	\\
0.09	&	0	&	art::TriggerResults\_TriggerResult\_\_DAQ.	\\
\hline
76,703.25 & 1.0 & Total\\
\end{dunetable}

\begin{dunetable}
[Compressed size for Reconstructed data - 7 GeV beam event]
{rrl}
{tab:comp-reco-data-size}
{Compressed size for Reconstructed data - 7 GeV beam events}
Size in kBytes&	Fraction&		Data Product Name	\\
4,218,536	&	0.185	&	recob::Wires\_digitwire\_\_DecoderandReco.	\\
2,236,432	&	0.098	&	recob::Hits\_hitpdune\_\_DecoderandReco.	\\
2,102,520	&	0.092	&	recob::Hits\_gaushit\_\_DecoderandReco.	\\
2,052,796	&	0.090	&	recob::Hits\_linecluster\_\_DecoderandReco.	\\
2,020,575	&	0.089	&	artdaq::Fragments\_daq\_ContainerPHOTON\_DAQ.	\\
1,532,502	&	0.067	&	raw::OpDetWaveforms\_ssprawdecoder\_internal\_DecoderandReco.	\\
1,018,088	&	0.045	&	anab::Calorimetrys\_pandoracalo\_\_DecoderandReco.	\\
873,797&	0.038	&	recob::Tracks\_pandoraTrack\_\_DecoderandReco.	\\
806,513&	0.035	&	anab::Calorimetrys\_pmtrackcalo\_\_DecoderandReco.	\\
555,775&	0.024	&	recob::SpacePoints\_pandora\_\_DecoderandReco.	\\
479,599&	0.021	&	recob::Tracks\_pmtrack\_\_DecoderandReco.	\\
414,824&	0.018	&	raw::OpDetWaveforms\_ssprawdecoder\_external\_DecoderandReco.	\\
391,791&	0.017	&	recob::Hitrecob::Trackrecob::TrackHitMetaart::Assns\_pmtrack\_\_DecoderandReco.	\\
379,553&	0.017	&	recob::SpacePoints\_pmtrack\_\_DecoderandReco.	\\
310,021&	0.014	&	recob::SpacePoints\_reco3d\_pre\_DecoderandReco.	\\
260,143&	0.011	&	recob::Hitrecob::SpacePointvoidart::Assns\_hitpdune\_\_DecoderandReco.	\\
250,175&	0.011	&	recob::Hitrecob::SpacePointvoidart::Assns\_reco3d\_\_DecoderandReco.	\\
229,711&	0.01	&	recob::SpacePoints\_reco3d\_noreg\_DecoderandReco.	\\
218,874&	0.01	&	recob::SpacePoints\_reco3d\_\_DecoderandReco.	\\
200,618&	0.009	&	recob::Hitrecob::SpacePointvoidart::Assns\_pandora\_\_DecoderandReco.	\\
2,407,376&0.106	&Smaller Objects	\\
\hline
22,759,597&	1.000		&Total	\\
\end{dunetable}

<<<<<<< HEAD
\begin{dunetable}[Algorithm timing for 7 GeV beam event]{lrr}{tab:comp-reco-data-time}{Algorithm timing for 7 GeV beam events.  Smaller processes not shown for clarity. A 10 event job used 2.7 GB of memory to do this reconstruction.}
Processing step&Average CPU time, sec\\									
source:RootInput(read)	&	0.2	\\
%decode:timingrawdecoder:TimingRawDecoder	&	0.0	\\
%decode:ssprawdecoder:SSPRawDecoder	&	0.3	\\
decode:tpcrawdecoder:PDSPTPCRawDecoder	&	15.9	\\
%decode:crtrawdecoder:CRTRawDecoder	&	0.0	\\
%decode:ctbrawdecoder:PDSPCTBRawDecoder	&	0.0	\\
decode:beamevent:BeamEvent	&	0.7	\\
decode:caldata:DataPrepModule	&	89.1	\\
decode:wclsdatasp:WireCellToolkit	&	113.3	\\
%decode:digitwire:EventButcher	&	0.8	\\
decode:gaushit:GausHitFinder	&	20.7	\\
decode:reco3d:SpacePointSolver	&	18.0	\\
decode:hitpdune:DisambigFromSpacePoints	&	23.0	\\
decode:linecluster:LineCluster	&	3.9	\\
decode:pandora:StandardPandora	&	93.2	\\
decode:pandoraTrack:LArPandoraTrackCreation	&	12.3	\\
decode:pandoraShower:LArPandoraShowerCreation	&	5.2	\\
decode:pandoracalo:Calorimetry	&	5.8	\\
%decode:pandorapid:Chi2ParticleID	&	0.0	\\
decode:pmtrack:PMAlgTrackMaker	&	142.0	\\
decode:pmtrackcalo:Calorimetry	&	6.1	\\
%decode:pmtrackpid:Chi2ParticleID	&	0.0	\\
%decode:ophitInternal:OpHitFinder	&	0.0	\\
%decode:ophitExternal:OpHitFinder	&	0.0	\\
%decode:opflashInternal:OpFlashFinder	&	0.0	\\
%decode:opflashExternal:OpFlashFinder	&	0.0	\\
%[art]:TriggerResults:TriggerResultInserter	&	0.0	\\
%end_path:out1:RootOutput	&	0.0	\\
cout1:RootOutput(write)	&	3.3\\	
Total&	503.7\\
\end{dunetable}
=======
\begin{dunetable}[Reconstruction times for 7 GeV beam event]{lrr}{tab:comp-raw-data-size}{Reconstruction times for 7 GeV beam events}
Processing step &	Avg CPUtime(sec) 	\\			
\hline			
Fullevent	&	503.7	\\
\hline			
source:RootInput(read)	&	0.2	\\
%decode:timingrawdecoder:TimingRawDecoder	&	0.0	\\
decode:ssprawdecoder:SSPRawDecoder	&	0.3	\\
decode:tpcrawdecoder:PDSPTPCRawDecoder	&	15.9	\\
%decode:crtrawdecoder:CRTRawDecoder	&	0.0	\\
%decode:ctbrawdecoder:PDSPCTBRawDecoder	&	0.0	\\
decode:beamevent:BeamEvent	&	0.7	\\
decode:caldata:DataPrepModule	&	89.1	\\
decode:wclsdatasp:WireCellToolkit	&	113.3	\\
decode:digitwire:EventButcher	&	0.8	\\
decode:gaushit:GausHitFinder	&	20.7	\\
decode:reco3d:SpacePointSolver	&	18.0	\\
decode:hitpdune:DisambigFromSpacePoints	&	23.0	\\
decode:linecluster:LineCluster	&	3.9	\\
decode:pandora:StandardPandora	&	93.2	\\
decode:pandoraTrack:LArPandoraTrackCreation	&	12.3	\\
decode:pandoraShower:LArPandoraShowerCreation	&	5.2	\\
decode:pandoracalo:Calorimetry	&	5.8	\\
decode:pandorapid:Chi2ParticleID	&	0.0	\\
decode:pmtrack:PMAlgTrackMaker	&	142.0	\\
decode:pmtrackcalo:Calorimetry	&	6.1	\\
%decode:pmtrackpid:Chi2ParticleID	&	0.0	\\
%decode:ophitInternal:OpHitFinder	&	0.0	\\
%decode:ophitExternal:OpHitFinder	&	0.0	\\
%decode:opflashInternal:OpFlashFinder	&	0.0	\\
%decode:opflashExternal:OpFlashFinder	&	0.0	\\
%[art]:TriggerResults:TriggerResultInserter	&	0.0	\\
%end_path:out1:RootOutput	&	0.0	\\
:out1:RootOutput(write)	&	3.3	\\\end{dunetable}
>>>>>>> 0409f6327748e16759137f14bdf97ff9e368f272





 




\section{Processing Infrastructure for Reconstruction and Simulation}
\label{ch-comp-processing}
ProtoDUNE makes use of computing resources internationally through the Open Science Grid and the parallel infrastructure set up for WLCG in Europe.  In 2018, significant effort was put into integrating European sites into the DUNE reconstruction and simulation processing with very positive results.  
Figure \ref{fig:ch-exec-comp-cpupie} shows the distribution of production jobs worldwide in November and December 2018 during the main reconstruction pass.  FNAL and CERN as the host laboratories made the largest contributions but significant resources were also made available from the UK through integration with GridPP and in the Czech republic through FZU and at IN2P3 in France. 

\begin{figure}[htp]
\centering
%\subfloat[]{
%\includegraphics[height=2.5in]{comp-dunepro_pdsp_keepup.png}%
\includegraphics[height=4in]{graphics/comp-vo-summary.png}
%}
%\vspace{1cm}
%\subfloat[]{
%\includegraphics[height=2.3in]{comp-dunepro_mcc11.png}%
%\includegraphics[height=2.3in]{comp-dunepro_mcc11_legend.png}
%}
\caption{CPU wall-time for November 2018 during ProtoDUNE-SP reconstruction showing multiple site contributions.  The major contributions were from FNAL, CERN, many UK institutions and FZU.}
\label{fig:ch-exec-comp-cpupie}
\end{figure}

\begin{dunetable}
[Data storage  and CPU needs for reconstruction of ProtoDUNE test beam data]
{llrrrr}{tab:exec-comp-needs}{Data storage and CPU needs for reconstruction of ProtoDUNE-SP test beam data taken in 2018 and projections for 2019-2021.  We assume two copies of raw data are stored and that each event is reconstructed twice.  Analysis and simulation are estimated to be of order the same CPU use as reconstruction based on the 2018 experience.}%\rowtitlestyle
Detector& value &
2018&
2019&
2020&
2021\\
&&As built\\
\hline
SP&
Events, M&
15.1&
13.0&
6.5&
40.5\\
&
Raw data, TB&
1047&
2239&
1120&
2799\\
&
Reco data, TB&
2094&
4479&
2239&
5599\\
&
CPU, MH&
5.0&
4.3&
2.2&
13.5\\
\hline
DP&
Events, M&
0.0&
101.1&
56.2&
119.9\\
&
Raw data, TB&
0&
809&
449&
1799\\
&
Reco data, TB&
0&
1617&
899&
3598\\
&
CPU, MH&
0.0&
33.7&
18.7&
40.0\\
\hline
total&
Events, M&
15.1&
114.0&
62.6&
160.4\\
&2x
Raw data, TB&
2094&
6096&
3138&
9197\\
&
Reco data, TB&
2094&
6096&
3138&
9197\\
total&Storage, TB
4188&
12193&
6276&
18394\\
&
Reco CPU, MH&
5.0&
38.0&
20.9&
53.5\\
&
Analysis CPU, MH&
5.0&
40.0&
40.0&
40.0\\
Total&CPU, MH&
10.0\\
78.0\\
60.9\\
93.5\\
\end{dunetable}

\subsection{Lessons learned}

\begin{itemize}
    \item Data and simulation challenges led to a reasonably mature and robust model for acquiring, storing and cataloging the main data stream. 
    \item The experiment was able to integrate multiple existing grid sites and make use of substantial opportunistic resources.  This allowed initial processing of data within one month of the end of the run.
    \item Substantial but successful effort went into signal processing. 
    \item Reconstruction algorithms are not perfect but sufficient for studies of detector performance and calibration. 
    \item Beam information was successfully integrated into the processing through the \dword{ifbeam} database.
    \item Auxiliary information from, for example, slow controls was not integrated into processing due to lack of personpower.  This led to dependence on hand input of running conditions by shift personnel and offline incorporation of that information into the data catalog. 
\end{itemize}

Overall, the ProtoDUNE-SP data taking and processing was a success but overly dependent on doing things "by hand" as automated processes were not always in place. 

\subsection{Near future}

Table \ref{tab:exec-comp-needs} summarizes the known resource usage for 2018 and projections for 2019-2021.  The collaboration has requested a substantial test beam run for both single and dual phase detectors in 2021.  The \dshort{csc} views this run as a first production test for the full DUNE computing infrastructure. 





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Data Model for the Far and Near Detectors}		
\label{ch:exec-comp-mod}

%%%%%%%%%%%%%%%%%%%%
\subsection{Introduction}	
\label{ch:exec-comp-mod-int}
In parallel with the ProtoDUNE-SP data, a joint Data Model task force was formed by the DAQ and Computing Consortia to lay a framework for the full near and far DUNE detectors. 
The Data Model task force grappled with the problems of efficiently triggering, reading out and storing data from an enormous detector on multiple time scales.

They defined major concepts.

\begin{description}

\item{Configuration:} set of parameters that define the persisted, expected detector state. Globally, this corresponds to a desirable state for the detector, capable of providing data of physics or calibration quality. Each component of the detector may have its own configuration.
 
\item{Run:} Period of time over which data has been collected across some set of desired components in a consistent configuration.
 
\item{Subrun:} Period of time within a run over which data has been collected across some set of desired components in a consistent configuration. The set of desired components in a subrun must be a subset of the desired components for a run, and is the set of components over which data is expected.
 
(Time-based rollovers of runs and subruns may be automatic. Differences of subrun and run due to configuration or changes in the desired components will be tracked by the DAQ, and may be either manual or automatic.)
 
\item{Trigger:} data from the desired components in that subrun over a window of time (a ?readout window?). This would typically be centered around a trigger time, and is what is recorded by the DAQ. The readout window may be subdivided into ?frames? as determined by the DAQ.
 
\item{Event:} subset of a trigger isolated in time and space containing an independent interaction in the detector. Events may overlap in space or time, within the same trigger. This is generally determined by the offline, based on reconstruction of data in a frame.

\end{description}

These definitions are intended to allow triggering, recording and reconstruction of interactions in subsets of the detector. While the whole detector (or time window) can produce enormous amounts of data, any individual interaction is expected to span a reasonably short time and spatial volume. A data model that can isolate individual interactions  allows efficient storage and reconstruction of interactions. 


The main data stream will be augmented by beam, slow controls, \dword{daq} configuration and calibration information. 

This work continues and informs  the  joint  calibration, \dshort{csc} and \dshort{daq} designs.


\section{Consortium Organization}

%The DUNE computing and software consortium was formed in October 2018.  
%%%%%%%%%%%%%%%%%%%%
%\subsection{Requirements}	
%\label{ch:exec-comp-mod-req}


%%%%%%%%%%%
%\subsubsection{How Physics Drives}
%\label{ch:exec-comp-mod-req-phys-drv}


%%%%%%%%%%%
%\subsubsection{Oscillation Analyses}
%\label{ch:exec-comp-mod-req-osc}


%%%%%%%%%%%
%\subsubsection{Cross Sections}
%\label{ch:exec-comp-mod-req-xsec}


%%%%%%%%%%%
%\subsubsection{Other Drivers}
%\label{ch:exec-comp-mod-req-oth-drv}


%%%%%%%%%%%%%%%%%%%%
%\subsection{Interfaces with Other Projects}	
%\label{ch:exec-comp-mod-intfc}


%%%%%%%%%%%
%\subsubsection{DAQ}
%\label{ch:exec-comp-mod-intfc-daq}


%%%%%%%%%%%
%\subsubsection{Calibration}
%\label{ch:exec-comp-mod-intfc-calib}

%Why is this cutting off here on the page?

%%%%%%%%%%%
%\subsubsection{Physics}
%\label{ch:exec-comp-mod-intfc-phys}

%Why is this cutting off here on the page?

%%%%%%%%%%%%%%%%%%%%
%\subsection{Use cases}	
%\label{ch:exec-comp-mod-use}


%%%%%%%%%%%
%\subsubsection{Discussion of detector options (SP,DP,ND? )}
%\label{ch:exec-comp-mod-use-opt}


%%%%%%%%%%%
%\subsubsection{Data acquisition}
%\label{ch:exec-comp-mod-use-daq}


%%%%%%%%%%%
%\subsubsection{Data Quality}
%\label{ch:exec-comp-mod-use-dq}


%%%%%%%%%%%
%\subsubsection{Reconstruction}
%\label{ch:exec-comp-mod-use-reco}


%%%%%%%%%%%
%\subsubsection{Calibration}
%\label{ch:exec-comp-mod-use-calib}


%%%%%%%%%%%
%\section{Simulation}
%\label{ch:exec-comp-mod-use-sim}


%%%%%%%%%%%
%\section{Analysis}
%\label{ch:exec-comp-mod-use-anls}


%%%%%%%%%%%%%%%%%%%%
%\subsection{Existing Infrastructure}	
%\label{ch:exec-comp-mod-infr}


%%%%%%%%%%%
%\subsubsection{sam/enstore/eos/castor}
%\label{ch:exec-comp-mod-infr-stor}


%%%%%%%%%%%
%\subsubsection{Grid}
%\label{ch:exec-comp-mod-infr-gr}


%%%%%%%%%%%
%\subsubsection{Databases}
%\label{ch:exec-comp-mod-infr-db}


%%%%%%%%%%%%%%%%%%%%
%\subsection{ProtoDUNE Experience}	
%\label{ch:exec-comp-mod-pdune}


%%%%%%%%%%%%%%%%%%%%
%\subsection{Evolving Infrastructure}	
%%\label{ch:exec-comp-mod-evlv}
%Why is this cutting off here on the page?

%%%%%%%%%%%
%\subsubsection{rucio}
%\label{ch:exec-comp-mod-evlv-ruc}
%Why is this cutting off here on the page?

%%%%%%%%%%%
%\subsubsection{Load Management}
%\label{ch:exec-comp-mod-evlv-load}


%%%%%%%%%%%
%\subsubsection{?.}
%\label{ch:exec-comp-mod-evlv-}


%%%%%%%%%%%%%%%%%%%%
%\subsection{Novel Architectures}	
%\label{ch:exec-comp-mod-nov}

%%%%%%%%%%%
%\subsubsection{HPC}
%\label{ch:exec-comp-mod-nov-hpc}

%%%%%%%%%%%%%%%%%%%%
%\subsection{Authentication}	
%\label{ch:exec-comp-mod-auth}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Software}		
%\label{ch:exec-comp-sw}


%%%%%%%%%%%%%%%%%%%%
%\subsection{Introduction}	
%\label{ch:exec-comp-sw-int}


%%%%%%%%%%%%%%%%%%%%
%\subsection{Existing Packages}	
%\label{ch:exec-comp-sw-int-pkg}


%%%%%%%%%%%
%\subsubsection{GEANT4}
%\label{ch:exec-comp-sw-int-gnt}


%%%%%%%%%%%
%\subsubsection{ROOT}
%\label{ch:exec-comp-sw-int-root}
%hy is this cutting off here on the page?

%%%%%%%%%%%
%\subsubsection{art}
%\label{ch:exec-comp-sw-int-art}


%%%%%%%%%%%%%%%%%%%%
%\subsection{Evolving Packages}	
%\label{ch:exec-comp-sw-evpkg}


%%%%%%%%%%%
%\subsubsection{LArSoft}
%\label{ch:exec-comp-sw-evpkg-larsoft}


%%%%%%%%%%%
%\subsubsection{Wirecell}
%\label{ch:exec-comp-sw-evpkg-wcell}


%%%%%%%%%%%
%\subsubsection{GENIE}
%\label{ch:exec-comp-sw-evpkg-genie}


%%%%%%%%%%%%%%%%%%%%
%\subsection{DUNE-specific Software}	
%\label{ch:exec-comp-sw-evpkg-spec}


%%%%%%%%%%%%%%%%%%%%
%\subsection{Novel Architectures}	
%\label{ch:exec-comp-sw-novarch}

%%%%%%%%%%%
%\subsubsection{Machine Learning}
%\label{ch:exec-comp-sw-novarch-mach}
%Why is this cutting off here on the page?
%%%%%%%%%%%
%\subsubsection{Vectorization}
%\label{ch:exec-comp-sw-novarch-vec}

%%%%%%%%%%%%%%%%%%%%
%\subsection{Development Environment	}
%\label{ch:exec-comp-sw-devenv}

%%%%%%%%%%%
%\subsubsection{Environment Specifications}
%\label{ch:exec-comp-sw-devenv-spec}


%%%%%%%%%%%
%\subsubsection{Code and Configuration Management}
%\label{ch:exec-comp-sw-devenv-mgmt}


%%%%%%%%%%%
%\subsubsection{Validation}
%\label{ch:exec-comp-sw-devenv-val}


%%%%%%%%%%%%%%%%%%%%
%\subsection{Training and Communication}	
%\label{ch:exec-comp-sw-train}
%Why is this cutting off here on the page?

%%%%%%%%%%%%%%%%%%%%
%\subsection{Lessons from ProtoDUNE}	
%\label{ch:exec-comp-sw-lessons}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Resources and Governance}		
\label{ch:exec-comp-gov}

The Computing and Software effort is now a DUNE Consortium.  Docdb 12751 \lcite{bib:docdb12751} describes the governance structure for the Consortium.  The Consortium coordinates effort across the collaboration but funding comes from collaborating institutions, laboratories and national funding agencies. 

The consortium has an overall Consortium Leader. The Consortium Leader is responsible for the sub-system deliverables and represents the consortium to the overall DUNE collaboration.

In addition there  are Technical Leads to act as the overall project managers for the consortium. The Technical Leads report to the overall Consortium Leader.
Computing has both a Host Laboratory Technical Project Lead, responsible for coordination with the DUNE Project and host lab and an International Technical Lead responsible for coordination with institutions outside the US.?
As with other DUNE consortia, the consortium is responsible for assigning a provisional division of institutional
responsibilities for computing resources, deliverables and operations, amongst the participating institutions. This division of responsibilities must account for the resources that are likely to be available. The internally agreed division of responsibilities needs to be presented to the Technical Board, which will then make a recommendation to the collaboration management for approval.



\subsection{Scope of the Consortium}
The Computing and Software Consortium (\dword{csc}) is mainly concerned with the infrastructure and resources for offline computing.  Algorithm development resides within the Physics groups while online systems at experimental sites are governed by the Data Acquisition and Cryogenics Instrumentation and Slow Controls Consortia. These groups coordinate closely to assure that the full chain of data acquisition, processing and analysis works. Formal interfaces with these groups are described in Docdb 7123 (DAQ)\lcite{bib:docdb7123} and Docdb 7126 (CISC)\lcite{bib:docdb7126}.

The consortium operates at two levels; at the hardware level, where generic resources can be provided as in kind contributions to the collaboration and at the human level, where individuals and groups contribute to the development of common software infrastructure. 

\subsection{Hardware}
As noted above, the collaboration has already made use of substantial global resources through the \dword{wlcg} and \dword{osg} grid mechanisms. As the Consortium evolves, institutions and collaborating nations will be asked to make formal pledges of resources (both CPU and storage) and those resources will be accounted for and considered in-kind contributions to the collaboration.
As illustrated above, several international partners are already making substantial contributions. We are currently integrating additional large national facilities. Most resources are currently opportunistic but Fermilab and CERN have committed several thousand cores and several PB of disk and the UK reserves 10\% of GridPP resources for non-LHC experiments, an allocation that DUNE has already benefited from.

\begin{dunetable}
[DUNE S+C Consortium Members as of February 20 19]{lll}{tab:exec-comp-consortium}{DUNE S+C Consortium Members as of February 2019, -- indicates sites not yet integrated into production computing. }%\rowtitlestyle
Institution& Country & Integrated\\
KISTI	&	Korea	&	--	\\
Nikhef	&	NL	&	Yes	\\
Edinburgh	&	UK	&	Yes	\\
GridPP	&	UK	&	Yes	\\
Manchester	&	UK	&	Yes	\\
RAL/STFC	&	UK	&	Yes	\\
Argonne	&	USA	&	--	\\
BNL	&	USA	&	Yes	\\
Cincinnati	&	USA	& data management	\\
Colorado State	&	USA	&databases	\\
CU Boulder	&	USA	&	Yes	\\
Fermilab	&	USA	&	Yes	\\
Florida 	&	USA	&	production	\\
LBNL	&	USA	&	Yes	\\
Minnesota	&	USA	&	databases	\\
Northern Illinois Univ.	&	USA	& --	\\
Notre Dame	&	USA	&	\dword{larsoft}	\\
Oregon State University	&	USA	&	management	\\
Tennessee	&	USA	&	--\\
Texas, Austin	&	USA	&	--\\
\end{dunetable}

\subsection{People}

The \ldshort{csc} has (or will have) subgroups for the following areas.  Highlights of some of the ongoing projects are detailed in subsequent sections. 

\begin{itemize}
    \item 
Collaborative Tools
\item Data Storage and Management
\item Databases 
\item Production and Processing 
\item Workflow Management
\item Data Quality Monitoring 
\item Software Release Management 
\item Core Software led by a Software Architect
\item Advanced Architectures
\item Algorithm liaisons
\item Networking
\end{itemize}
%%%%%%%%%%%%%%%%%%%%
\section{Cooperative Work with Other Collaborations	}
\label{ch:exec-comp-gov-coop}

The HEP computing community has come together to form an HEP Software Foundation (\dshort{hsf})\cite{Alves:2017she} which through working groups, workshops and white-papers is guiding the next generation of shared HEP software.  DUNE's time-scale, where we are in the planning and evaluation phase, is almost perfect for us to contribute to and benefit from these efforts.  Our overall strategy for computing infrastructure is to carefully evaluate existing and proposed field-wide solutions, to participate in their design where they are useful and to only build our own solutions where the common solutions do not fit.  This section describes some of these common activities. 



\subsection{\dword{larsoft} for event reconstruction}

The \dword{larsoft} \lcite{Snider:2017wjd} reconstruction package is shared by a collaboration of LAr neutrino experiments.  MicroBooNE, SBND, DUNE and others share in the development of a common core software framework with customization for each experiment. The existence of this software suite and prior efforts by other experiments is what made the rapid reconstruction of the ProtoDUNE-SP data possible.  DUNE will be a major contributor to  the future evolution of this package, in particular in introducing full multi-threading to allow parallel reconstruction of parts of large events in anticipation of the extremely large events expected from the full detector. 



\subsection{Relation to WLCG/OSG}
The Worldwide LHC Computing Grid organization (\dshort{wlcg})\cite{WLCG}, which currently combines the resource and infrastructure missions for the LHC experiments, has proposed a future governance structure that splits the dedicated resource provision for LHC experiments from the general middleware infrastructure used to access those resources.  This Scientific Computing Infrastructure (\dshort{sci}) is already used by many other experiments worldwide.  In a white paper submitted to the European Strategy Group in December 2018\lcite{http://wlcg-docs.web.cern.ch/wlcg-docs/technical\_documents/HEP-Computing-Evolution.pdf}, a formal Scientific Computing Infrastructure organization is proposed. As part of the transition to that structure the DUNE collaboration has been provisionally invited to join the WLCG with observer status and participate in the Grid Management Board. The goal of our participation is to have input into the technical decisions on global computing infrastructure while contributing to that infrastructure. 

Areas of collaboration include:



\subsection{Rucio Development and Extension}

 \dword{rucio}
\cite{Barisits:2019fyl}
is a data management system originally developed by the ATLAS collaboration and now an open-source project.  DUNE has chosen to use \dword{rucio} for our large scale data movement.  In the short term it is being combined with the \dword{sam} data catalog used by Fermilab experiments.  DUNE collaborators at FNAL and in the UK are actively collaborating in the \dword{rucio} project, adding value for DUNE but also the wider effort.


There is a global \dword{rucio} team which now includes Fermilab SCD staff, DUNE collaborators, and CMS collaborators in addition to the core ATLAS developers who wrote it in the first place.  Consortium members have started collaborative work on several projects.   These include (a) making object stores (such as Amazon S3 and compatible utilities) work with Rucio.  There is a large object store in the UK on which DUNE has a sizable allocation.  (b) Monitoring  and administration of the \dword{rucio} system, leveraging the Landscape system at Fermilab.  (c) Designing a  data description engine that can be used as a replacement for the SAM system we currently use.

Rucio has already been shown to be a powerful and useful tool for getting defined datasets from point A to point B.  Our initial observation is that \dword{rucio} is a good solution for file localization but is missing the detailed tools for data description and granular dataset definition available in the current \dword{sam} system.  The rapidly varying conditions in the test beam have highlighted a need for a sophisticated data description database interfaced to \dword{rucio}'s location functions. 

In addition the data management team has a design decision to be made with regards to Rucio.  LHC experiments such as ATLAS and CMS work with disk stores and tape stores that are independent of each other.  This is different than the dCache model used by most Fermilab experiments in which most of dCache is a caching frontend for a tape store.  Efficient integration of caching into the \dword{rucio} model will be an important component for DUNE unless  \dword{larsoft}we can afford to have most data on disk to avoid staging.

\subsection{Testing of New Storage Technologies and Interfaces}

There is currently a Data Organization, Management, and Access (\dword{doma}) taskforce working in the larger HEP community\cite{Berzano:2018xaa}
 in which several DUNE collaborators are involved. There are task forces for authorization, caching, third party copy, hierarchical storage, and quality of service. All of these are of interest to DUNE as they will determine the long-term standards for common computing infrastructure in the field. 
In particular, the authorization issues are of much importance to DUNE and are covered in subsection \ref{ch-comp-auth}


\subsection{Data Management and Retention Policy Development}



There is a data life cycle built into the DUNE data model.  Particularly old simulations and histograms, old commissioning data, don't have to live forever.  We should have automated ways to enforce the data life cycle model.  We need to plan in an automated way as possible to organize the structure of lower storage so that the various retention types are stored separately for easy deletion if necessary.  This should be configured in such a way to minimize the possibility for human error.

\subsection{Authentication and Authorization Security and Interoperability}\label{ch-comp-auth}

Within the next 2-3 years we expect the global HEP community to make significant changes in the methods of authentication and authorization of computing and storage.  %One of the new ideas already in testing for storage is something called "capability-based authentication" and employs technologies such as "SciTokens" or "Macaroons".  There are also various proposals to replace the long-standing VOMS mechanism with a different type of authentication.
Over that period, DUNE needs to collaborate with the US and European HEP computing communities on improved authentication methods  that will allow secure but transparent access to storage and other resources such as logbooks and code repositories across the collaboration. The current model where individuals need to authenticate through different mechanisms for access to US and European resources is already a roadblock to efficient integration  of personnel and storage. 





%\todo{\verbatim{Add reference to http://wlcg-docs.web.cern.ch/wlcg-docs/technical_documents/HEP-Computing-Evolution.pdf}}



\subsection{Evaluations of other important infrastructure}

The DUNE S+C effort is still evaluating major infrastructure components, notably databases and workflow management systems.

For databases\cite{Laycock:2019ynk}, the Fermilab Conditions Database is being used for the first run of ProtoDUNE but the Belle II\cite{Ritter:2018jxh} system supported by BNL is also being considered for subsequent runs. 

For workflow management, we are evaluating \dword{dirac}\cite{Falabella:2016waj} and plan to investigate PANDA \cite{Megino:2017ywl}. Both of these systems are used by multiple LHC and non-LHC experiments and are already being integrated with \dword{rucio}. 


\section{Conclusion}

The DUNE Software and Computing efforts have already undergone a substantial test with the successful run of ProtoDUNE-SP, including demonstration of data movement to storage at 2GB/s, reconstruction with high quality algorithms of the full test beam sample and the start of analysis of the multiple PB of reconstructed data. 

The Consortium is now working with the global HEP computing community to evaluate and specify modern infrastructure that will serve the needs of DUNE and the rest of the community.  We plan to collaborate wherever possible with other experiments where we have common technical challenges. However, the extremely large but simple events generated by Liquid Argon TPC's, even with short readouts, present a unique challenge. 

At this point our major goals to do careful reviews of available and potential tools, to build collaborations and to find the resources necessary to do a large number of ambitious projects. 
%%%%%%%%%%%%%%%%%%%%
%\subsection{Resource Needs}	
%\label{ch:exec-comp-gov-res}

%%%%%%%%%%%
%\subsubsection{Hardware}
%\label{ch:exec-comp-gov-res-hw}

%%%%%%%%%%%
%\subsubsection{Personnel}
%\label{ch:exec-comp-gov-res-hum}

%%%%%%%%%%%%%%%%%%%%
%\subsection{Contribution Models}	
%\label{ch:exec-comp-gov-contrib}
%Why is this cutting off here on the page?

%%%%%%%%%%%%%%%%%%%%
%\subsection{Technical Decision Governance}	
%\label{ch:exec-comp-gov-tech}

%%%%%%%%%%%%%%%%%%%%
%\subsection{Resource Decision Governance	}
%\label{ch:exec-comp-gov-resdec}

%%%%%%%%%%%%%%%%%%%%
%\subsection{Project Management}	
%\label{ch:exec-comp-gov-pm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  REMOVE THESE %%%%%%%%%%%%%
%\printglossaries
%\end{document}

%\ignore{
%  REFERENCES 
%
%@article{Snider:2017wjd,
%      author         = "Snider, E. L. and Petrillo, G.",
%      title          = "{LArSoft: Toolkit for Simulation, Reconstruction and
%                        Analysis of Liquid Argon TPC Neutrino Detectors}",
%      booktitle      = "{Proceedings, 22nd International Conference on Computing
%                        in High Energy and Nuclear Physics (CHEP2016): San
%                        Francisco, CA, October 14-16, 2016}",
%      journal        = "J. Phys. Conf. Ser.",
%      volume         = "898",
%      year           = "2017",
%      number         = "4",
%      pages          = "042057",
%      doi            = "10.1088/1742-6596/898/4/042057",
%      reportNumber   = "FERMILAB-CONF-17-052-CD",
%      SLACcitation   = "%%CITATION = 00462,898,042057;%%"
%}
%
%REFERENCES
%
%@article{Snider:2017wjd,
%      author         = "Snider, E. L. and Petrillo, G.",
%      title          = "{LArSoft: Toolkit for Simulation, Reconstruction and
%                        Analysis of Liquid Argon TPC Neutrino Detectors}",
%      booktitle      = "{Proceedings, 22nd International Conference on Computing
%                        in High Energy and Nuclear Physics (CHEP2016): San
%                        Francisco, CA, October 14-16, 2016}",
%      journal        = "J. Phys. Conf. Ser.",
%      volume         = "898",
%      year           = "2017",
%      number         = "4",
%      pages          = "042057",
%      doi            = "10.1088/1742-6596/898/4/042057",
%      reportNumber   = "FERMILAB-CONF-17-052-CD",
%      SLACcitation   = "%%CITATION = 00462,898,042057;%%"
%}
%DQM
%
%https://docs.dunescience.org/cgi-bin/private/ShowDocument?docid=10551
%
%DOMA
%
%@article{Berzano:2018xaa,
%      author         = "Berzano, Dario and others",
%      title          = "{HEP Software Foundation Community White Paper Working
%                        Group -- Data Organization, Management and Access (DOMA)}",
%      year           = "2018",
%      eprint         = "1812.00761",
%      archivePrefix  = "arXiv",
%      primaryClass   = "physics.comp-ph",
%      reportNumber   = "HSF-CWP-2017-04, FERMILAB-PUB-18-671-CD",
%      SLACcitation   = "%%CITATION = ARXIV:1812.00761;%%"
%}
%
%
%%%% contains utf-8, see: https://inspirehep.net/info/faq/general#utf8
%%%% add \usepackage[utf8]{inputenc} to your latex preamble
%
%@article{Laycock:2019ynk,
%      author         = "Bracko, Marko and Clemencic, Marco and Dykstra, Dave and
%                        Formica, Andrea and Govi, Giacomo and Jouvin, Michel and
%                        Lange, David and Laycock, Paul and Wood, Lynn",
%      title          = "{HEP Software Foundation Community White Paper Working
%                        Group  Conditions Data}",
%      year           = "2019",
%      eprint         = "1901.05429",
%      archivePrefix  = "arXiv",
%      primaryClass   = "physics.comp-ph",
%      reportNumber   = "FERMILAB-PUB-19-044-CD",
%      SLACcitation   = "%%CITATION = ARXIV:1901.05429;%%"
%}
%
%@article{Calafiura:2018rwe,
%      author         = "Calafiura, Paolo and others",
%      editor         = "Hegner, Benedikt and Kowalkowski, Jim and Sexton-Kennedy,
%                        Elizabeth",
%      title          = "{HEP Software Foundation Community White Paper Working
%                        Group - Data Processing Frameworks}",
%      year           = "2018",
%      eprint         = "1812.07861",
%      archivePrefix  = "arXiv",
%      primaryClass   = "physics.comp-ph",
%      reportNumber   = "HSF-CWP-2017-08, FERMILAB-PUB-18-693-CD",
%      SLACcitation   = "%%CITATION = ARXIV:1812.07861;%%"
%}
%
%@article{Berzano:2018xaa,
%      author         = "Berzano, Dario and others",
%      title          = "{HEP Software Foundation Community White Paper Working
%                        Group -- Data Organization, Management and Access (DOMA)}",
%      year           = "2018",
%      eprint         = "1812.00761",
%      archivePrefix  = "arXiv",
%      primaryClass   = "physics.comp-ph",
%      reportNumber   = "HSF-CWP-2017-04, FERMILAB-PUB-18-671-CD",
%      SLACcitation   = "%%CITATION = ARXIV:1812.00761;%%"
%}
%
%
%%%% contains utf-8, see: https://inspirehep.net/info/faq/general#utf8
%%%% add \usepackage[utf8]{inputenc} to your latex preamble
%
%@article{Bellis:2018hej,
%      author         = "Bellis, Matthew and others",
%      title          = "{HEP Software Foundation Community White Paper Working
%                        Group  Visualization}",
%      year           = "2018",
%      eprint         = "1811.10309",
%      archivePrefix  = "arXiv",
%      primaryClass   = "physics.comp-ph",
%      reportNumber   = "HSF-CWP-2017-15, FERMILAB-PUB-18-710-CD",
%      SLACcitation   = "%%CITATION = ARXIV:1811.10309;%%"
%}
%
%@article{Hildreth:2018tsn,
%      author         = "Hildreth, M. D. and others",
%      title          = "{HEP Software Foundation Community White Paper Working
%                        Group - Data and Software Preservation to Enable Reuse}",
%      year           = "2018",
%      eprint         = "1810.01191",
%      archivePrefix  = "arXiv",
%      primaryClass   = "physics.comp-ph",
%      reportNumber   = "HSF-CWP-2017-06, FERMILAB-FN-1060-CD",
%      SLACcitation   = "%%CITATION = ARXIV:1810.01191;%%"
%}
%
%@article{Albertsson:2018maf,
%      author         = "Albertsson, Kim and others",
%      title          = "{Machine Learning in High Energy Physics Community White
%                        Paper}",
%      booktitle      = "{Proceedings, 18th International Workshop on Advanced
%                        Computing and Analysis Techniques in Physics Research
%                        (ACAT 2017): Seattle, WA, USA, August 21-25, 2017}",
%      journal        = "J. Phys. Conf. Ser.",
%      volume         = "1085",
%      year           = "2018",
%      number         = "2",
%      pages          = "022008",
%      doi            = "10.1088/1742-6596/1085/2/022008",
%      eprint         = "1807.02876",
%      archivePrefix  = "arXiv",
%      primaryClass   = "physics.comp-ph",
%      reportNumber   = "FERMILAB-PUB-18-318-CD-DI-PPD",
%      SLACcitation   = "%%CITATION = ARXIV:1807.02876;%%"
%}
%
%@article{Berzano:2018krv,
%      author         = "Berzano, Dario and others",
%      title          = "{HEP Software Foundation Community White Paper Working
%                        Group - Training, Staffing and Careers}",
%      collaboration  = "HEP Software Foundation",
%      year           = "2018",
%      eprint         = "1807.02875",
%      archivePrefix  = "arXiv",
%      primaryClass   = "physics.ed-ph",
%      reportNumber   = "HSF-CWP-2017-02",
%      SLACcitation   = "%%CITATION = ARXIV:1807.02875;%%"
%}
%
%@article{Bauerdick:2018qjx,
%      author         = "Bauerdick, Lothar and others",
%      editor         = "Neubauer, Mark S.",
%      title          = "{HEP Software Foundation Community White Paper Working
%                        Group - Data Analysis and Interpretation}",
%      collaboration  = "HEP Software Foundation",
%      year           = "2018",
%      eprint         = "1804.03983",
%      archivePrefix  = "arXiv",
%      primaryClass   = "physics.comp-ph",
%      reportNumber   = "HSF-CWP-2017-05, FERMILAB-FN-1057-CD-PPD",
%      SLACcitation   = "%%CITATION = ARXIV:1804.03983;%%"
%}
%
%@article{Apostolakis:2018ieg,
%      author         = "Apostolakis, J and others",
%      editor         = "Elvira, V and Harvey, J",
%      title          = "{HEP Software Foundation Community White Paper Working
%                        Group - Detector Simulation}",
%      collaboration  = "HEP Software Foundation",
%      year           = "2018",
%      eprint         = "1803.04165",
%      archivePrefix  = "arXiv",
%      primaryClass   = "physics.comp-ph",
%      reportNumber   = "HSF-CWP-2017-07, FERMILAB-FN-1054-CD",
%      SLACcitation   = "%%CITATION = ARXIV:1803.04165;%%"
%}
%
%@article{Albrecht:2018iur,
%      author         = "Albrecht, Johannes and others",
%      title          = "{HEP Community White Paper on Software Trigger and Event
%                        Reconstruction}",
%      year           = "2018",
%      eprint         = "1802.08638",
%      archivePrefix  = "arXiv",
%      primaryClass   = "physics.comp-ph",
%      reportNumber   = "FERMILAB-PUB-18-071-CD",
%      SLACcitation   = "%%CITATION = ARXIV:1802.08638;%%"
%}
%
%@article{Albrecht:2018zgl,
%      author         = "Albrecht, Johannes and others",
%      title          = "{HEP Community White Paper on Software Trigger and Event
%                        Reconstruction: Executive Summary}",
%      year           = "2018",
%      eprint         = "1802.08640",
%      archivePrefix  = "arXiv",
%      primaryClass   = "physics.comp-ph",
%      reportNumber   = "FERMILAB-PUB-18-072-CD",
%      SLACcitation   = "%%CITATION = ARXIV:1802.08640;%%"
%}
%
%@article{Couturier:2017cgq,
%      author         = "Couturier, Benjamin and others",
%      title          = "{HEP Software Foundation Community White Paper Working
%                        Group - Software Development, Deployment and Validation}",
%      year           = "2017",
%      eprint         = "1712.07959",
%      archivePrefix  = "arXiv",
%      primaryClass   = "physics.comp-ph",
%      reportNumber   = "HSF-CWP-2017-13",
%      SLACcitation   = "%%CITATION = ARXIV:1712.07959;%%"
%}
%
%@article{Alves:2017she,
%      author         = "Albrecht, Johannes and others",
%      title          = "{A Roadmap for HEP Software and Computing R\&D for the
%                        2020s}",
%      collaboration  = "HEP Software Foundation",
%      journal        = "Comput. Softw. Big Sci.",
%      volume         = "3",
%      year           = "2019",
%      number         = "1",
%      pages          = "7",
%      doi            = "10.1007/s41781-018-0018-8",
%      eprint         = "1712.06982",
%      archivePrefix  = "arXiv",
%      primaryClass   = "physics.comp-ph",
%      reportNumber   = "HSF-CWP-2017-01, HSF-CWP-2017-001,
%                        FERMILAB-PUB-17-607-CD",
%      SLACcitation   = "%%CITATION = ARXIV:1712.06982;%%"
%}
%
%
%
%
%
%
%
%
%
%
%
%
%}% end ignore
